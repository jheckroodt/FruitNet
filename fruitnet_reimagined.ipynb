{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fruitnet_reimagined.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R93IVx_rQBlV"
      },
      "source": [
        "<h1>Libraries</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xc-O8sSqQ63m"
      },
      "source": [
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import h5py\r\n",
        "\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHWTZPd4QGFX"
      },
      "source": [
        "<h1>Mounting the Drive</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHtTp8XAREHX",
        "outputId": "5eea013a-6ab6-4d00-d7db-a95d87bc4f39"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ppeRGiA_-U1"
      },
      "source": [
        "<h1>Dataset Operations</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-F8CNw5AAzr"
      },
      "source": [
        "def build_mini_batches(X,Y,size):\r\n",
        "    '''\r\n",
        "    Description: splits the given dataset, X, into mini batches, preserving the correspondance between the dataset and its corresponding labels\r\n",
        "\r\n",
        "    Inputs:\r\n",
        "    - X: the dataset to be split into mini batches\r\n",
        "    - Y: the labels corresponding to X\r\n",
        "    - size: the size of each mini batch\r\n",
        "\r\n",
        "    Returns:\r\n",
        "    - mini_batches: a list of tuples of the form (Xt,Yt), where Xt is a mini batch of X, and Yt are the labels corresponding to Xt\r\n",
        "    '''\r\n",
        "\r\n",
        "    #reshape the dataset X from a conventional image dataset shape to a fully connected input shape\r\n",
        "    X=X.reshape(X.shape[0],X.shape[1],X.shape[2])\r\n",
        "    X=X.reshape(X.shape[0],X.shape[1]*X.shape[2])\r\n",
        "    X=X.T\r\n",
        "    m=X.shape[1]\r\n",
        "    assert size<=m\r\n",
        "\r\n",
        "    #create the shuffled order of the dataset\r\n",
        "    permutation=np.random.permutation(m)\r\n",
        "\r\n",
        "    #shuffle the dataset, as well as its corresponding labels\r\n",
        "    X=X[:,permutation]\r\n",
        "    Y=Y[:,permutation]\r\n",
        "\r\n",
        "    #instantiate the mini batch list\r\n",
        "    mini_batches=[]\r\n",
        "\r\n",
        "    #create the mini batches of size 'size'\r\n",
        "    whole_batches=m//size\r\n",
        "    for t in range(whole_batches):\r\n",
        "        X_mini_batch=X[:,t*size:(t+1)*size]\r\n",
        "        Y_mini_batch=Y[:,t*size:(t+1)*size]\r\n",
        "        mini_batches.append((X_mini_batch,Y_mini_batch))\r\n",
        "\r\n",
        "    #if there are any elements of the dataset that have not been arranged into a mini batch, arrange said elements into a mini batch\r\n",
        "    if m%size!=0:\r\n",
        "        X_mini_batch=X[:,whole_batches*size:m]\r\n",
        "        Y_mini_batch=Y[:,whole_batches*size:m]\r\n",
        "        mini_batches.append((X_mini_batch,Y_mini_batch))\r\n",
        "    \r\n",
        "    return mini_batches\r\n",
        "\r\n",
        "def split_into_strips(Xt,strips):\r\n",
        "    '''\r\n",
        "    Description: split the mini batch Xt into subsets that comprise strips of the original image to which Xt corresponds\r\n",
        "\r\n",
        "    Inputs:\r\n",
        "    - Xt: mini batch to split into the aforementioned subsets\r\n",
        "    - strips: the number of subsets into which to split our mini batches\r\n",
        "\r\n",
        "    Returns:\r\n",
        "    - tuple(split_mini_batch): a tuple of the form (Xt1,Xt2,...,Xtstrips), where each Xti (for i ranging from 1 to strips) is an appropriate subset of Xt\r\n",
        "    '''\r\n",
        "\r\n",
        "    #confirm the validity of the number of strips chosen\r\n",
        "    if int(np.sqrt(Xt.shape[0]))%strips==0:\r\n",
        "\r\n",
        "        #establish the width of each strip (or the height, rather)\r\n",
        "        width=Xt.shape[0]//strips\r\n",
        "\r\n",
        "        #instantiate the list of subsets of Xt\r\n",
        "        split_mini_batch=[]\r\n",
        "\r\n",
        "        #split Xt into subsets and organize into split_mini_batch appropriately\r\n",
        "        for i in range(strips):\r\n",
        "            split_mini_batch.append(Xt[i*width:(i+1)*width,:])\r\n",
        "    \r\n",
        "        return tuple(split_mini_batch)\r\n",
        "    else:\r\n",
        "        raise ValueError('you have elected a number of strips that results in strips of unequal width. please re-enter the number of strips you\\'d like, and try again.')\r\n",
        "\r\n",
        "def reshuffle_split_mini_batches(X,Y,hp):\r\n",
        "    '''\r\n",
        "    Description: splits the dataset X into minibatches, and splits each of these mini batches into appropriate subsets, as described in the description of split_into_strips\r\n",
        "\r\n",
        "    Inputs:\r\n",
        "    - X: dataset to be split into mini batches and subsets\r\n",
        "    - Y: the labels corresponding to X (throughout the operations this function performs, correspondence between X and Y is preserved)\r\n",
        "    - hp: the hyperparameters in accordance with which this function operates\r\n",
        "\r\n",
        "    Returns:\r\n",
        "    - split_mini_batches: a list of tuples of the form ((Xt1,Xt2,...,Xtstrips),Yt), where (Xt1,Xt2,...,Xtstrips) is as described above, and Yt is the set of labels corresponding to (Xt1,Xt2,...,Xtstrips)\r\n",
        "    '''\r\n",
        "\r\n",
        "    #split our dataset into mini batches\r\n",
        "    mini_batches=build_mini_batches(X,Y,hp['mini_batch_size'])\r\n",
        "\r\n",
        "    #instantiate our return variable\r\n",
        "    split_mini_batches=[]\r\n",
        "\r\n",
        "    #split the above mini batches into subsets and arrange into split_mini_batches (with Yt) accordingly\r\n",
        "    for (Xt,Yt) in mini_batches:\r\n",
        "        split_mini_batches.append((split_into_strips(Xt,hp['strips']),Yt))\r\n",
        "    \r\n",
        "    return split_mini_batches"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOFZ2dieQJ87"
      },
      "source": [
        "<h1>Initialization Operations</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0qF7s3nQ48u"
      },
      "source": [
        "def initialize_parameters(layer_dims,initialization):\r\n",
        "    '''\r\n",
        "    Description: initializes the parameters of a fully connected network whose architecture is specified by layer_dims\r\n",
        "\r\n",
        "    Inputs:\r\n",
        "    - layer_dims: a list of the number of nodes in each of the layers of the network whose parameters we wish to initialize\r\n",
        "    - initialization: the method in accordance with which our parameters are initialized\r\n",
        "\r\n",
        "    Returns:\r\n",
        "    - parameters: a dictionary containing all of the parameters relevant to our network\r\n",
        "    '''\r\n",
        "\r\n",
        "    #establish the number of layers in the network\r\n",
        "    L=len(layer_dims)\r\n",
        "\r\n",
        "    #instantiate our parameter dictionary\r\n",
        "    parameters={}\r\n",
        "\r\n",
        "    #in accordance with our preferred initialization method, initialize each of the relevant parameters\r\n",
        "    if initialization=='None':\r\n",
        "        for l in range(1,L):\r\n",
        "            parameters['W'+str(l)]=0.01*np.random.rand(layer_dims[l],layer_dims[l-1])\r\n",
        "            parameters['gamma'+str(l)]=np.ones((layer_dims[l],1))\r\n",
        "            parameters['beta'+str(l)]=np.zeros((layer_dims[l],1))\r\n",
        "    elif initialization=='He':\r\n",
        "        for l in range(1,L):\r\n",
        "            const=np.sqrt(2/layer_dims[l-1])\r\n",
        "            parameters['W'+str(l)]=const*np.random.rand(layer_dims[l],layer_dims[l-1])\r\n",
        "            parameters['gamma'+str(l)]=np.ones((layer_dims[l],1))\r\n",
        "            parameters['beta'+str(l)]=np.zeros((layer_dims[l],1))\r\n",
        "    elif initialization=='Xavier':\r\n",
        "        for l in range(1,L):\r\n",
        "            const=np.sqrt(1/layer_dims[l-1])\r\n",
        "            parameters['W'+str(l)]=const*np.random.rand(layer_dims[l],layer_dims[l-1])\r\n",
        "            parameters['gamma'+str(l)]=np.ones((layer_dims[l],1))\r\n",
        "            parameters['beta'+str(l)]=np.zeros((layer_dims[l],1))\r\n",
        "    elif initialization=='Other':\r\n",
        "        for l in range(1,L):\r\n",
        "            const=np.sqrt(2/(layer_dims[l]+layer_dims[l-1]))\r\n",
        "            parameters['W'+str(l)]=const*np.random.rand(layer_dims[l],layer_dims[l-1])\r\n",
        "            parameters['gamma'+str(l)]=np.ones((layer_dims[l],1))\r\n",
        "            parameters['beta'+str(l)]=np.zeros((layer_dims[l],1))\r\n",
        "    \r\n",
        "    return parameters\r\n",
        "\r\n",
        "def build_nets(split_mini_batch,hp):\r\n",
        "    '''\r\n",
        "    Description: initializes the parameters necessary for all of the subnets, as well as all of the supernets in our model\r\n",
        "\r\n",
        "    Inputs:\r\n",
        "    - split_mini_batch: a single mini batch from the dataset whose contents we wish to model (used to establish the input layer shape of our subnets)\r\n",
        "    - hp: the hyperparameters in accordance with which the generation of the subnets and supernet takes place\r\n",
        "\r\n",
        "    Returns:\r\n",
        "    - subnet_parameters: a dictionary of dictionaries, each (nested) dictionary defining the parameters for a different subnet\r\n",
        "    - supnet parameters: a dictionary defining the parameters for the supernet\r\n",
        "    '''\r\n",
        "\r\n",
        "    #extract a single subset from the given mini batch (these subsets are what is ultimately fed into our subnets as inputs), as well as its corresponding labels\r\n",
        "    X=split_mini_batch[0][0]\r\n",
        "    Y=split_mini_batch[1]\r\n",
        "\r\n",
        "    #instantiate our subnet parameters dictionary\r\n",
        "    subnet_parameters={}\r\n",
        "\r\n",
        "    #define the dimensions of our subnets, in accordance with hp['subnet_hidden'], which contained the number of hidden nodes in a hidden layer, if there are any hidden layers\r\n",
        "    if hp['subnet_hidden']==[]:\r\n",
        "        layer_dims=[X.shape[0],Y.shape[0]]\r\n",
        "    else:\r\n",
        "        layer_dims=[X.shape[0]]+hp['subnet_hidden']+[Y.shape[0]]\r\n",
        "\r\n",
        "    #in accordance with the dimensions of our subnets, define their parameters\r\n",
        "    for i in range(hp['strips']):\r\n",
        "        subnet_parameters['subnet'+str(i+1)]=initialize_parameters(layer_dims,hp['initialization'])\r\n",
        "\r\n",
        "    #repeat the above process for the supernet\r\n",
        "    if hp['supnet_hidden']==[]:\r\n",
        "        layer_dims=[hp['strips']*Y.shape[0],Y.shape[0]]\r\n",
        "    else:\r\n",
        "        layer_dims=[hp['strips']*Y.shape[0]]+hp['supnet_hidden']+[Y.shape[0]]\r\n",
        "    supnet_parameters=initialize_parameters(layer_dims,hp['initialization'])\r\n",
        "\r\n",
        "    return subnet_parameters,supnet_parameters\r\n",
        "\r\n",
        "def normalize_inputs(X):\r\n",
        "    '''\r\n",
        "    Description: normalize the dataset X by dividing each of its elements by 255 (since the range of pixel intensities of an image is given by [0,255])\r\n",
        "\r\n",
        "    Inputs:\r\n",
        "    - X: the dataset to be noramlized\r\n",
        "\r\n",
        "    Returns:\r\n",
        "    - X divided by 255\r\n",
        "    '''\r\n",
        "\r\n",
        "    return X/255"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZNy-1qcQONr"
      },
      "source": [
        "<h1>Network Operations</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ttbdnRhQ5gZ"
      },
      "source": [
        "def sigmoid(z,backward=False):\r\n",
        "    '''\r\n",
        "    Description: the sigmoid activation function\r\n",
        "\r\n",
        "    Inputs:\r\n",
        "    - z: the np.array() object to which we wish to apply the sigmoid activation function\r\n",
        "    - backward: dictates whether or not the function is being called during the forward pass or the backward pass\r\n",
        "\r\n",
        "    Returns:\r\n",
        "    - the np.array() object resulting from the sigmoid activation function being applied to z\r\n",
        "\r\n",
        "    '''\r\n",
        "\r\n",
        "    if backward:\r\n",
        "        return (1/(1+np.exp(-z)))*(1-(1/(1+np.exp(-z))))\r\n",
        "    else:\r\n",
        "        return 1/(1+np.exp(-z))\r\n",
        "\r\n",
        "def relu(z,backward=False):\r\n",
        "    '''\r\n",
        "    Description: the ReLU activation function\r\n",
        "\r\n",
        "    Inputs:\r\n",
        "    - z: the np.array() object to which we wish to apply the ReLU activation function\r\n",
        "    - backward: dictates whether or not the function is being called during the forward pass or the backward pass\r\n",
        "\r\n",
        "    Returns:\r\n",
        "    - the np.array() object resulting from the ReLU activation function being applied to z\r\n",
        "    '''\r\n",
        "    if backward:\r\n",
        "        return np.where(z<0,0,1)\r\n",
        "    else:\r\n",
        "        return np.where(z<0,0,z)\r\n",
        "\r\n",
        "def softmax(z):\r\n",
        "    '''\r\n",
        "    Description: the softmax activation function (this activation function may only be, and is necessarily, applied to the final layer of both the subnet and the supernet)\r\n",
        "\r\n",
        "    Inputs:\r\n",
        "    - z: the np.array() object to which we wish to apply the softmax activation function\r\n",
        "\r\n",
        "    Returns:\r\n",
        "    - the np.array() object resulting from the softmax activation function being applied to z\r\n",
        "    '''\r\n",
        "    magnitude=np.sum(np.exp(z),axis=0,keepdims=True)\r\n",
        "    return np.exp(z)/magnitude\r\n",
        "\r\n",
        "def concat(z,classes,m=None):\r\n",
        "    '''\r\n",
        "    Description: concatenates the activations produced by the subnets to form an input for the supernet.\r\n",
        "\r\n",
        "    Inputs:\r\n",
        "    - z: a list of activations produced by the subnets, to be concatenated into a supernet input\r\n",
        "    - classes: the number of output nodes in a given subnet\r\n",
        "    - m: the size of a mini batch on which we are training the subnets\r\n",
        "\r\n",
        "    Returns:\r\n",
        "    - new_z: the elements of z, concatenated into an appropriately shaped input to the supernet\r\n",
        "\r\n",
        "    Notes: this function was also used in a previous variation of the project, where subnets operated slightly differently, hence m may be regarded as dictating whether or not this function is being called in the forward pass or the backward pass\r\n",
        "    '''\r\n",
        "\r\n",
        "    #check the size of the mini batch (if m==None, then the function is being called during the backward pass, a possibility which is omitted in this iteration of the project, as mentioned by the note above)\r\n",
        "    if m==None:\r\n",
        "        subnet_dA=[]\r\n",
        "        for i in range(z.shape[0]//classes):\r\n",
        "            subnet_dA.append(z[i*classes:(i+1)*classes,:])\r\n",
        "        return subnet_dA\r\n",
        "    else:\r\n",
        "\r\n",
        "        #instantiate our return variable\r\n",
        "        new_z=np.zeros((len(z)*classes,m))\r\n",
        "\r\n",
        "        #arrange the elements of z into our return variable\r\n",
        "        for i in range(len(z)):\r\n",
        "            new_z[i*classes:(i+1)*classes,:]=z[i]\r\n",
        "\r\n",
        "        return new_z\r\n",
        "\r\n",
        "def propagate_forward(X,parameters,activations,epsilon=1e-8):\r\n",
        "    '''\r\n",
        "    Description: propagates the mini batch X forward across a network with parameters 'parameters'\r\n",
        "\r\n",
        "    Inputs:\r\n",
        "    - X: mini batch to be propagated forward\r\n",
        "    - parameters: the parameters of the network along which X is propagated\r\n",
        "    - activations: the activation function of each of the layers in the network with an activation function\r\n",
        "    - epsilon: prevents division by 0\r\n",
        "\r\n",
        "    Returns:\r\n",
        "    - cache_list: the list of values caches for the purposes of performing the backward pass\r\n",
        "    - A_prev: the final layer activation produced by the network in question\r\n",
        "    '''\r\n",
        "\r\n",
        "    #assign X to A_prev so that we may utilize A_prev in our eventual loop through the network layers\r\n",
        "    A_prev=X\r\n",
        "\r\n",
        "    #instiate the list in which the caches values from each layer will be stored\r\n",
        "    cache_list=[]\r\n",
        "\r\n",
        "    #identify the number of layers in the network\r\n",
        "    L=len(parameters)//3\r\n",
        "\r\n",
        "    #loop through the layers in the network\r\n",
        "    for l in range(L):\r\n",
        "\r\n",
        "        #apply the weights to the input to the layer\r\n",
        "        Z=np.dot(parameters['W'+str(l+1)],A_prev)\r\n",
        "\r\n",
        "        #apply batch normalization to the resulting np.array() object\r\n",
        "        mu=np.mean(Z,axis=1,keepdims=True)\r\n",
        "        std=np.var(Z,axis=1,keepdims=True)\r\n",
        "        Zhat=(Z-mu)/np.sqrt(std+epsilon)\r\n",
        "        Ztilde=parameters['gamma'+str(l+1)]*Zhat+parameters['beta'+str(l+1)]\r\n",
        "\r\n",
        "        #cache the relevant values before updating A_prev\r\n",
        "        cache_list.append((Ztilde,Zhat,mu,Z,std,A_prev))\r\n",
        "\r\n",
        "        #update A_prev\r\n",
        "        A_prev=activations[l](Ztilde)\r\n",
        "\r\n",
        "    return cache_list,A_prev\r\n",
        "\r\n",
        "def compute_cost(AL,Y):\r\n",
        "    '''\r\n",
        "    Description: computes the (softmax) cost of a network output\r\n",
        "\r\n",
        "    Inputs:\r\n",
        "    - AL: the output produced by the network in question\r\n",
        "    - Y: the true labels to which the outputs correspond\r\n",
        "\r\n",
        "    Returns:\r\n",
        "    - the aforementioned cost\r\n",
        "    '''\r\n",
        "\r\n",
        "    #retrieve the size of the mini batch for which the cost is being computed\r\n",
        "    m=Y.shape[1]\r\n",
        "\r\n",
        "    #compute the loss of each element\r\n",
        "    loss=-np.sum(Y*np.log(AL),axis=0,keepdims=True)\r\n",
        "\r\n",
        "    #average the element-wise loss to obtain the cost\r\n",
        "    return np.squeeze(np.sum(loss,axis=1,keepdims=True))/m\r\n",
        "\r\n",
        "def propagate_backward(Y,AL,cache_list,parameters,activations,epsilon=1e-8):\r\n",
        "    '''\r\n",
        "    Description: performs the backward pass along a network with parameters 'parameters'\r\n",
        "\r\n",
        "    Inputs:\r\n",
        "    - Y: the true labels corresponding to the mini batch on which the network is currently being trained\r\n",
        "    - AL: the activation produced by the network in question, corresponding to Y\r\n",
        "    - cache_list: one of the outputs of propagate_forward, the cache list needed to perform back propagation\r\n",
        "    - parameters: the parameters of the network in question\r\n",
        "    - activations: the activation functions of each of the layers in the network with an activation function\r\n",
        "    - epsilon: avoids division by zero\r\n",
        "\r\n",
        "    Returns:\r\n",
        "    - grads: the gradients of each of the parameters in the network\r\n",
        "    '''\r\n",
        "\r\n",
        "    #instantiate our gradients dictionary\r\n",
        "    grads={}\r\n",
        "\r\n",
        "    #retrieve the size of the mini batch on which the network is currently being trained\r\n",
        "    m=Y.shape[1]\r\n",
        "\r\n",
        "    #retrieve the number of layers in the network\r\n",
        "    L=len(parameters)//3\r\n",
        "\r\n",
        "    #loop through the entire network\r\n",
        "    for l in reversed(range(L)):\r\n",
        "\r\n",
        "        #retrieve the relevant values from the cache corresponding to the current layer\r\n",
        "        (Ztilde,Zhat,mu,Z,std,A_prev)=cache_list[l]\r\n",
        "\r\n",
        "        #compute the gradient of the variable to which we apply the activation function of the current layer (during the forward pass)\r\n",
        "        if l==L-1:\r\n",
        "            dZtilde_loss=AL-Y\r\n",
        "            dZtilde=dZtilde_loss/m\r\n",
        "        else:\r\n",
        "            dZtilde=dA*activations[l](Ztilde,backward=True)\r\n",
        "\r\n",
        "        #propagate backwards through the batch normalization portion of the layer\r\n",
        "        dZhat=dZtilde*parameters['gamma'+str(l+1)]\r\n",
        "        dgamma=np.sum(dZtilde*Zhat,axis=1,keepdims=True)\r\n",
        "        dbeta=np.sum(dZtilde,axis=1,keepdims=True)\r\n",
        "        dstd=np.sum(dZhat*((mu-Z)/(2*(std+epsilon)**(3/2))),axis=1,keepdims=True)\r\n",
        "        dmu=-np.sum(dZhat*(1/np.sqrt(std+epsilon)),axis=1,keepdims=True)\r\n",
        "        dZ=dZhat*(1/np.sqrt(std+epsilon))+(2*dstd*(Z-mu))/m+dmu/m\r\n",
        "\r\n",
        "        #propagate backward through the weighted protion of the layer\r\n",
        "        dW=np.dot(m*dZ,A_prev.T)\r\n",
        "\r\n",
        "        #prepare for back propagation through the preceding layer (if there is a preceding layer)\r\n",
        "        dA=np.dot(parameters['W'+str(l+1)].T,dZ)\r\n",
        "\r\n",
        "        #amend our gradient dictionary\r\n",
        "        grads['dgamma'+str(l+1)]=dgamma\r\n",
        "        grads['dbeta'+str(l+1)]=dbeta\r\n",
        "        grads['dW'+str(l+1)]=dW\r\n",
        "    \r\n",
        "    return grads\r\n",
        "\r\n",
        "def update_parameters(parameters,grads,learning_rate,epoch_num):\r\n",
        "    '''\r\n",
        "    Description: updates the parameters of a network in accordance with the gradients obtained while propagating backward along said network (using learning rate decay)\r\n",
        "\r\n",
        "    Inputs:\r\n",
        "    - parameters: the parameters of the network whose parameters we wish to update\r\n",
        "    - grads: the gradients corresponding to each of the elements of parameters\r\n",
        "    - learning_rate: the initial learning rate\r\n",
        "    - epoch_num: the 'how many-th' epoch we're currently executing\r\n",
        "\r\n",
        "    Returns:\r\n",
        "    - new_parameters: the updated parameters\r\n",
        "    '''\r\n",
        "\r\n",
        "    #perform learning rate decay first\r\n",
        "    lrate=learning_rate/(1+(1e-1)*epoch_num)\r\n",
        "\r\n",
        "    #retrieve the number of layers in the network\r\n",
        "    L=len(parameters)//3\r\n",
        "\r\n",
        "    #instantiate our return variable\r\n",
        "    new_parameters={}\r\n",
        "\r\n",
        "    #populate our return variable with the relevant updated parameters\r\n",
        "    for l in range(L):\r\n",
        "        new_parameters['W'+str(l+1)]=parameters['W'+str(l+1)]-lrate*grads['dW'+str(l+1)]\r\n",
        "        new_parameters['gamma'+str(l+1)]=parameters['gamma'+str(l+1)]-lrate*grads['dgamma'+str(l+1)]\r\n",
        "        new_parameters['beta'+str(l+1)]=parameters['beta'+str(l+1)]-lrate*grads['dbeta'+str(l+1)]\r\n",
        "\r\n",
        "    return new_parameters"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZbYySY_RK1C"
      },
      "source": [
        "<h1>Compilation Operation(s)</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TdIHADcRfMZ"
      },
      "source": [
        "def train(X,Y,hp):\r\n",
        "    '''\r\n",
        "    Description: trains the network on X and Y\r\n",
        "\r\n",
        "    Inputs:\r\n",
        "    - X: the training set\r\n",
        "    - Y: the true labels corresponding to Y\r\n",
        "    - hp: the hyperparameters in accordance with which our training takes place\r\n",
        "\r\n",
        "    Returns:\r\n",
        "    - subnet_parameters: the dictionary containing the trained parameters of each of the subnets\r\n",
        "    - supnet_parameters: the dictionary containing the trained parameters corresponding to the supernet\r\n",
        "    - all_cache: the cache lists for each of the subnets, as well as the supernet, returned for the purpose of adjusting the value of each of the parameters so that said parameters become inference-ready\r\n",
        "    '''\r\n",
        "\r\n",
        "    #normalize the dataset\r\n",
        "    normalized_X=normalize_inputs(X)\r\n",
        "\r\n",
        "    #split the dataset into appropriate mini batches\r\n",
        "    split_mini_batches=reshuffle_split_mini_batches(normalized_X,Y,hp)\r\n",
        "\r\n",
        "    #instatiate the subnet parameters, as well as the supernet parameters\r\n",
        "    subnet_parameters,supnet_parameters=build_nets(split_mini_batches[0],hp)\r\n",
        "\r\n",
        "    #loop across each epoch\r\n",
        "    for e in range(hp['epochs']):\r\n",
        "\r\n",
        "        #loop across each mini batch\r\n",
        "        for (Xt,Yt) in split_mini_batches:\r\n",
        "\r\n",
        "            #instatiate the variables responsible for keeping track of the subnet activations, as well as the subnet cache lists\r\n",
        "            all_subnet_AL=[]\r\n",
        "            all_caches={}\r\n",
        "\r\n",
        "            #loop across each of the subsets of Xt to propagate the appropriate subset forward along the corresponding subnet, and subsequently performing the appropriate backward pass\r\n",
        "            for i in range(len(Xt)):\r\n",
        "                subnet_cache,subnet_AL=propagate_forward(Xt[i],subnet_parameters['subnet'+str(i+1)],hp['subnet_activations'])\r\n",
        "                all_subnet_AL.append(subnet_AL)\r\n",
        "                all_caches['subnet'+str(i+1)]=subnet_cache\r\n",
        "                subnet_grads=propagate_backward(Yt,subnet_AL,subnet_cache,subnet_parameters['subnet'+str(i+1)],hp['subnet_activations'])\r\n",
        "                subnet_parameters['subnet'+str(i+1)]=update_parameters(subnet_parameters['subnet'+str(i+1)],subnet_grads,hp['learning_rate'],e)\r\n",
        "            \r\n",
        "            #concatenate the subnet activations\r\n",
        "            Xt_concat=concat(all_subnet_AL,Yt.shape[0],m=Yt.shape[1])\r\n",
        "\r\n",
        "            #propagate the above concatenation forward along the supernet, keeping track of the resulting cache list\r\n",
        "            supnet_cache,supnet_AL=propagate_forward(Xt_concat,supnet_parameters,hp['supnet_activations'])\r\n",
        "            all_caches['supnet']=supnet_cache\r\n",
        "\r\n",
        "            #propagate backward along the supernet, and update the parameters of the supernet accordingly\r\n",
        "            supnet_grads=propagate_backward(Yt,supnet_AL,supnet_cache,supnet_parameters,hp['supnet_activations'])\r\n",
        "            supnet_parameters=update_parameters(supnet_parameters,supnet_grads,hp['learning_rate'],e)\r\n",
        "\r\n",
        "        #form predictions using the above produced activations\r\n",
        "        Y_hat=np.where(supnet_AL==np.amax(supnet_AL,axis=0,keepdims=True),1,0)\r\n",
        "\r\n",
        "        #print the results obtained from the above epoch to the terminal\r\n",
        "        print('--------------------------------------------------')\r\n",
        "        print('Cost after epoch '+str(e+1)+': '+str(compute_cost(supnet_AL,Yt)))\r\n",
        "        print('Error on final mini batch in this epoch: '+str(100*(np.linalg.norm(Yt-Y_hat)/np.sqrt(2*Yt.shape[1]))))\r\n",
        "        print('--------------------------------------------------')\r\n",
        "\r\n",
        "        #reshuffle the mini batches\r\n",
        "        split_mini_batches=reshuffle_split_mini_batches(normalized_X,Y,hp)\r\n",
        "\r\n",
        "    return subnet_parameters,supnet_parameters,all_caches"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_v4vXOiISLD2"
      },
      "source": [
        "<h1>Inference Operations</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjD2RemBSNso"
      },
      "source": [
        "def to_inf_params(parameters,cache_list,epsilon=1e-8):\r\n",
        "    '''\r\n",
        "    Description: converts the parameters 'parameters' to inference-ready parameters\r\n",
        "\r\n",
        "    Inputs:\r\n",
        "    - parameters: the parameters we wish to convert\r\n",
        "    - cache_list: a list containing the values we are to use to perform the above conversion\r\n",
        "    - epsilon: prevents division by zero\r\n",
        "\r\n",
        "    Returns:\r\n",
        "    - inf_params: the inference-ready variation of parameters\r\n",
        "    '''\r\n",
        "\r\n",
        "    #instantiate our return variable\r\n",
        "    inf_params={}\r\n",
        "\r\n",
        "    #retrieve the length of the network whose parameters we wish to convert\r\n",
        "    L=len(parameters)//3\r\n",
        "\r\n",
        "    #loop through each layer in the network\r\n",
        "    for l in range(L):\r\n",
        "\r\n",
        "        #retrieve the relevant cached values\r\n",
        "        (Ztilde,Zhat,mu,Z,std,A_prev)=cache_list[l]\r\n",
        "\r\n",
        "        #convert each of the parameters appropriately\r\n",
        "        inf_params['W'+str(l+1)]=parameters['W'+str(l+1)]\r\n",
        "        inf_params['gamma'+str(l+1)]=parameters['gamma'+str(l+1)]/np.sqrt(std+epsilon)\r\n",
        "        inf_params['beta'+str(l+1)]=parameters['beta'+str(l+1)]-mu*(parameters['gamma'+str(l+1)]/np.sqrt(std+epsilon))\r\n",
        "\r\n",
        "    return inf_params\r\n",
        "\r\n",
        "def inference_forward(X,parameters,activations,epsilon=1e-8):\r\n",
        "    '''\r\n",
        "    Description: propagates the test set X forward along the inference-ready network in question\r\n",
        "\r\n",
        "    Inputs:\r\n",
        "    - X: the test set\r\n",
        "    - parameters: inference-ready parameters corresponding to the network in question\r\n",
        "    - activations: the activation functions of each of the layers in our network\r\n",
        "    - epsilon: prevents division by zero\r\n",
        "\r\n",
        "    Returns:\r\n",
        "    - A_prev: the activations produced by the infere-ready network in question\r\n",
        "    '''\r\n",
        "\r\n",
        "    #assign X to A_prev so that we may propagate A_prev forward through the network in question\r\n",
        "    A_prev=X\r\n",
        "\r\n",
        "    #retrieve the length of the network in question\r\n",
        "    L=len(parameters)//3\r\n",
        "\r\n",
        "    #loop through each of the layers of the network\r\n",
        "    for l in range(L):\r\n",
        "\r\n",
        "        #apply the relevant weights\r\n",
        "        Z=np.dot(parameters['W'+str(l+1)],A_prev)\r\n",
        "\r\n",
        "        #apply the appropriate batch normalization parameters, as well as the activation function corresponding to the current layer\r\n",
        "        Ztilde=parameters['gamma'+str(l+1)]*Z+parameters['beta'+str(l+1)]\r\n",
        "        A_prev=activations[l](Ztilde)\r\n",
        "\r\n",
        "    return A_prev\r\n",
        "\r\n",
        "def perc_error(X,Y,subnet_parameters,supnet_parameters,hp):\r\n",
        "    '''\r\n",
        "    Description: compute the percentage error of our model on the test set\r\n",
        "\r\n",
        "    Inputs:\r\n",
        "    - X: the test set\r\n",
        "    - Y: the labels corresponding to X\r\n",
        "    - subnet_parameters: the inference-ready subnet parameters\r\n",
        "    - supnet_parameters: the inference-ready supernet parameters\r\n",
        "    - hp: the hyperparameters in accordance with which the error is computed\r\n",
        "\r\n",
        "    Returns:\r\n",
        "    - the percentage error on the test set\r\n",
        "    '''\r\n",
        "\r\n",
        "    #normalize the test set (this process needs to necessarily coincide with the normalization of the training set)\r\n",
        "    normalized_X=normalize_inputs(X)\r\n",
        "\r\n",
        "    #split the test set into appropriate mini batches\r\n",
        "    [(Xt,Yt)]=reshuffle_split_mini_batches(normalized_X,Y,{'mini_batch_size':Y.shape[1],'strips':hp['strips']})\r\n",
        "\r\n",
        "    #instantiate the subnet activation list\r\n",
        "    all_subnet_AL=[]\r\n",
        "\r\n",
        "    #loop across each of the subsets in the test set\r\n",
        "    for i in range(len(Xt)):\r\n",
        "\r\n",
        "        #propagate forward along each of the subnets and append the resulting activation to all_subnet_AL\r\n",
        "        subnet_AL=inference_forward(Xt[i],subnet_parameters['subnet'+str(i+1)],hp['subnet_activations'])\r\n",
        "        all_subnet_AL.append(subnet_AL)\r\n",
        "    \r\n",
        "    #concatenate the above activations\r\n",
        "    Xt_concat=concat(all_subnet_AL,Yt.shape[0],m=Yt.shape[1])\r\n",
        "\r\n",
        "    #feed the above concatenation into the supernet\r\n",
        "    supnet_AL=inference_forward(Xt_concat,supnet_parameters,hp['supnet_activations'])\r\n",
        "\r\n",
        "    #convert the activations produced by the supernet into a prediction\r\n",
        "    Y_hat=np.where(supnet_AL==np.amax(supnet_AL,axis=0,keepdims=True),1,0)\r\n",
        "    assert Y_hat.shape==Y.shape\r\n",
        "\r\n",
        "    return 100*(np.linalg.norm(Yt-Y_hat)/np.sqrt(2*Yt.shape[1]))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5B9J3oIaRfdU"
      },
      "source": [
        "<h1>FruitNet API</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BTBJwgUN24m"
      },
      "source": [
        "class FruitNet():\r\n",
        "\r\n",
        "    def __init__(self):\r\n",
        "        '''\r\n",
        "        Description: initializes the hyperparameters of our model\r\n",
        "\r\n",
        "        Inputs: none\r\n",
        "\r\n",
        "        Returns: none\r\n",
        "        '''\r\n",
        "\r\n",
        "        #instantiate hyperparameters\r\n",
        "        self.__hp={'learning_rate':0,\r\n",
        "                   'epochs':0,\r\n",
        "                   'mini_batch_size':0,\r\n",
        "                   'initialization':None,\r\n",
        "                   'strips':0,\r\n",
        "                   'subnet_names':[],\r\n",
        "                   'subnet_hidden':[],\r\n",
        "                   'subnet_activations':[],\r\n",
        "                   'supnet_names':[],\r\n",
        "                   'supnet_hidden':[],\r\n",
        "                   'supnet_activations':[]}\r\n",
        "    \r\n",
        "    def __raw_shuffle__(self,X,Y):\r\n",
        "        '''\r\n",
        "        Description: shuffles the dataset X, preserving the conventional image dataset shape\r\n",
        "\r\n",
        "        Inputs:\r\n",
        "        - X: the image dataset X, (m,w,w,1) (where w is the width of the square grayscale images in the dataset X)\r\n",
        "        - Y: the labels corresponding to X\r\n",
        "\r\n",
        "        Returns:\r\n",
        "        - shuffled_X: the dataset X, shuffled\r\n",
        "        - shuffled_Y: the labels Y, shuffled, so that shuffled_X and shuffled_Y maintain the correspodance present in X and Y\r\n",
        "        '''\r\n",
        "\r\n",
        "        #permute [1,2,...,m]\r\n",
        "        permutation=np.random.permutation(X.shape[0])\r\n",
        "\r\n",
        "        #shuffle X and Y in accordance with the above permutation\r\n",
        "        shuffled_X=X[permutation,:,:,:]\r\n",
        "        shuffled_Y=Y[:,permutation]\r\n",
        "\r\n",
        "        return shuffled_X,shuffled_Y\r\n",
        "\r\n",
        "    def loadData(self,filename,features,labels,split=0.05):\r\n",
        "        '''\r\n",
        "        Description: load data to our model\r\n",
        "\r\n",
        "        Inputs:\r\n",
        "        - filename: the h5py file from which our data comes\r\n",
        "        - features: the name of the features dataset (of shape (m,w,w,1), as above)\r\n",
        "        - labels: the corresponding labels dataset\r\n",
        "        - split: the split between the training set and the test set (as a value between 0 and 1, exclusive)\r\n",
        "\r\n",
        "        Returns: none\r\n",
        "        '''\r\n",
        "\r\n",
        "        #confirm the validity of split\r\n",
        "        assert 0<split<1\r\n",
        "\r\n",
        "        #retrieve data\r\n",
        "        try:\r\n",
        "            training_set=h5py.File(filename,'r')\r\n",
        "            X=training_set[features][:]\r\n",
        "            Y=training_set[labels][:]\r\n",
        "            training_set.close()\r\n",
        "        except:\r\n",
        "            raise NameError('you\\'ve entered an incorrect h5py filename, feature dataset name, or label dataset name, when loading in your data.')\r\n",
        "\r\n",
        "        #confirm validity of X dataset shape\r\n",
        "        assert len(X.shape)==4\r\n",
        "        assert X.shape[3]==1\r\n",
        "\r\n",
        "        #confirm validity of Y dataset shape\r\n",
        "        assert len(Y.shape)==2\r\n",
        "        assert X.shape[0]==Y.shape[1]\r\n",
        "\r\n",
        "        #shuffle datasets (maintaining correspondence)\r\n",
        "        shuffled_X,shuffled_Y=self.__raw_shuffle__(X,Y)\r\n",
        "\r\n",
        "        #establish cross-validation test set (and, therefore, training set) sizes\r\n",
        "        m=X.shape[0]\r\n",
        "        self.__test_quant=int(split*m)\r\n",
        "        self.__train_quant=m-self.__test_quant\r\n",
        "\r\n",
        "        #create cross-validation test set and training set using the above sizes\r\n",
        "        self.__X_train=shuffled_X[self.__test_quant:self.__test_quant+self.__train_quant,:,:,:]\r\n",
        "        self.__Y_train=shuffled_Y[:,self.__test_quant:self.__test_quant+self.__train_quant]\r\n",
        "        self.__X_test=shuffled_X[:self.__test_quant,:,:,:]\r\n",
        "        self.__Y_test=shuffled_Y[:,:self.__test_quant]\r\n",
        "\r\n",
        "    def __retrieve_single_split_example__(self,strips,view_data=True):\r\n",
        "        '''\r\n",
        "        Description: as the method name suggests, retrieves a single training example, split into appropriate subsections (as above), from the test set\r\n",
        "\r\n",
        "        Inputs:\r\n",
        "        - strips: the number of strips into which the aforementioned single test set element is split\r\n",
        "        - view_data: dictates whether or not the function is being called from viewData or __decoy_parameters__\r\n",
        "\r\n",
        "        Returns:\r\n",
        "        if view_data:\r\n",
        "            - X: the entire example taken from the test set\r\n",
        "            - Xt: a tuple containing the subsets into which X has been split\r\n",
        "            - Y: the label corresponding to X\r\n",
        "        else:\r\n",
        "            - a tuple containing the subsets into which X (as above) has been split, as well as the label corresponding to the aforementioned test set element X (denoted Yt in this case, instead of Y, as above)\r\n",
        "        '''\r\n",
        "\r\n",
        "        #select a random sample number\r\n",
        "        try:\r\n",
        "            sample=np.random.randint(0,self.__test_quant)\r\n",
        "        except:\r\n",
        "            raise UnboundLocalError('self.__test_quant has referenced before assignment, meaning you have not yet loaded in any data. please load in data and try again.')\r\n",
        "\r\n",
        "        #select the sample from the test set and split into regions\r\n",
        "        X=np.zeros((1,self.__X_test.shape[1],self.__X_test.shape[2],1))\r\n",
        "        Y=np.zeros((self.__Y_test.shape[0],1))\r\n",
        "        X[0,:,:,0]=self.__X_test[sample,:,:,0]\r\n",
        "        Y[:,0]=self.__Y_test[:,sample]\r\n",
        "        [(Xt,Yt)]=reshuffle_split_mini_batches(X,Y,{'mini_batch_size':1,'strips':strips})\r\n",
        "\r\n",
        "        #check why the function has been called and return accordingly\r\n",
        "        if view_data:\r\n",
        "            return X,Xt,Y\r\n",
        "        else:\r\n",
        "            return (Xt,Yt)\r\n",
        "\r\n",
        "\r\n",
        "    def viewData(self,strips):\r\n",
        "        '''\r\n",
        "        Description: used to visualize the splitting of a training example (technically, though, we use an example from the test set to this end)\r\n",
        "\r\n",
        "        Inputs:\r\n",
        "        - strips: the number of strips into which we wish to split a single test set element\r\n",
        "\r\n",
        "        Returns: none\r\n",
        "        '''\r\n",
        "\r\n",
        "        #confirm the validity of the number of strips\r\n",
        "        assert type(strips)==int\r\n",
        "        assert strips>1\r\n",
        "\r\n",
        "        #retrieve (split) item of data (from test set) corresponding to random_sample\r\n",
        "        X,Xt,Y=self.__retrieve_single_split_example__(strips)\r\n",
        "\r\n",
        "        #plot the sample, as well as the strips into which we've split it, and the corresponding label\r\n",
        "        fig=plt.figure()\r\n",
        "        axes=fig.subplots(strips+1,1)\r\n",
        "        axes[0].imshow(X[0,:,:,0])\r\n",
        "        for i in range(strips):\r\n",
        "            axes[i+1].imshow(Xt[i].reshape(X.shape[1]//strips,X.shape[2]))\r\n",
        "        print('The label corresponding to the above item of data (read from left to right) is given by:\\n'+str(Y.reshape(Y.shape[0],)))\r\n",
        "    \r\n",
        "    def addLayer(self,name,n_H,activation,net='sub'):\r\n",
        "        '''\r\n",
        "        Description: adds a hidden layer to the network of choice (bearing in mind that each subnet has the exact same architecture)\r\n",
        "\r\n",
        "        Inputs:\r\n",
        "        - name: the name of the layer we're adding to our model\r\n",
        "        - n_H: the number of hidden nodes in this layer\r\n",
        "        - net: dictates whether or not the layer is being added to the subnet architecture, or the supernet architecture\r\n",
        "\r\n",
        "        Returns: none\r\n",
        "        '''\r\n",
        "        \r\n",
        "        #confirm the validity of name, n_H, activation, and supnet\r\n",
        "        assert type(name)==str\r\n",
        "        assert type(n_H)==int\r\n",
        "        assert n_H>0\r\n",
        "        assert activation in [relu,sigmoid]\r\n",
        "        assert type(net)==str\r\n",
        "\r\n",
        "        #append the hyperparameters in accordance with the function parameters\r\n",
        "        if net=='sup':\r\n",
        "            assert name not in self.__hp['supnet_names']\r\n",
        "            self.__hp['supnet_names'].append(name)\r\n",
        "            self.__hp['supnet_hidden'].append(n_H)\r\n",
        "            self.__hp['supnet_activations'].append(activation)\r\n",
        "        elif net=='sub':\r\n",
        "            assert name not in self.__hp['subnet_names']\r\n",
        "            self.__hp['subnet_names'].append(name)\r\n",
        "            self.__hp['subnet_hidden'].append(n_H)\r\n",
        "            self.__hp['subnet_activations'].append(activation)\r\n",
        "        else:\r\n",
        "            raise ValueError('you have not selected a valid network to add a layer to. please re-enter the net string and try again.')\r\n",
        "    \r\n",
        "    def __decoy_parameters__(self):\r\n",
        "        '''\r\n",
        "        Description: generates a set of subtitute parameters for the purpose of producing a model summary\r\n",
        "\r\n",
        "        Inputs: none\r\n",
        "\r\n",
        "        Returns:\r\n",
        "        - the aforementioned set of substitute parameters\r\n",
        "        '''\r\n",
        "\r\n",
        "        #extract an example from the test set to preserve computational efficiency\r\n",
        "        single_example_mini_batch=self.__retrieve_single_split_example__(self.__hp['strips'],view_data=False)\r\n",
        "\r\n",
        "        #generate and return relevant parameters (NOT as class attributes, though)\r\n",
        "        return build_nets(single_example_mini_batch,self.__hp)\r\n",
        "\r\n",
        "    def modelSummary(self):\r\n",
        "        '''\r\n",
        "        Description: generates a model summary, and prints said summary to the terminal\r\n",
        "\r\n",
        "        Inputs:\r\n",
        "\r\n",
        "        Returns:\r\n",
        "        '''\r\n",
        "\r\n",
        "        #confirm that enough hyperparameters have been specified in order for a summary to be generated\r\n",
        "        assert self.__hp['strips']>0\r\n",
        "\r\n",
        "        #retrieve the parameters whose summary we wish to produce\r\n",
        "        try:\r\n",
        "            subnet=self.__subnet_parameters['subnet1']\r\n",
        "            supnet=self.__supnet_parameters\r\n",
        "        except:\r\n",
        "            subnets,supnet=self.__decoy_parameters__()\r\n",
        "            subnet=subnets['subnet1']\r\n",
        "        \r\n",
        "        #produce model summary header\r\n",
        "        print('==================================================')\r\n",
        "        print('NETWORK ARCHITECTURE SUMMARY')\r\n",
        "        print('==================================================\\n\\n')\r\n",
        "\r\n",
        "        #produce subnet summary\r\n",
        "        print('SUBNET SUMMARY')\r\n",
        "        print('--------------------------------------------------')\r\n",
        "        for i in range(len(subnet)//3+1):\r\n",
        "            if i==0:\r\n",
        "                print('Input Layer:')\r\n",
        "                print('- Name: N/A')\r\n",
        "                print('- No. of Input Nodes: '+str(subnet['W1'].shape[1]))\r\n",
        "                print('- Activation Function: N/A')\r\n",
        "                print('--------------------------------------------------')\r\n",
        "            else:\r\n",
        "                if i==len(subnet)//3:\r\n",
        "                    print('Output Layer:')\r\n",
        "                    print('- Name: N/A')\r\n",
        "                    print('- No. of Output Nodes: '+str(subnet['W'+str(i)].shape[0]))\r\n",
        "                    print('Activation Function: softmax')\r\n",
        "                    print('--------------------------------------------------')\r\n",
        "                else:\r\n",
        "                    print('Hidden Layer '+str(i)+':')\r\n",
        "                    print('- Name: '+str(self.__hp['subnet_names'][i-1]))\r\n",
        "                    print('- No. of Hidden Nodes: '+str(subnet['W'+str(i)].shape[0]))\r\n",
        "                    if self.__hp['subnet_activations'][i-1]==relu:\r\n",
        "                        print('- Activation Function: relu')\r\n",
        "                    else:\r\n",
        "                        print('- Activation Function: sigmoid')\r\n",
        "                    print('--------------------------------------------------')\r\n",
        "        print('\\n')\r\n",
        "\r\n",
        "        #produce supnet summary\r\n",
        "        print('SUPNET SUMMARY')\r\n",
        "        print('--------------------------------------------------')\r\n",
        "        for i in range(len(supnet)//3+1):\r\n",
        "            if i==0:\r\n",
        "                print('Input Layer:')\r\n",
        "                print('- Name: N/A')\r\n",
        "                print('- No. of Input Nodes: '+str(supnet['W1'].shape[1]))\r\n",
        "                print('- Activation Function: N/A')\r\n",
        "                print('--------------------------------------------------')\r\n",
        "            else:\r\n",
        "                if i==len(supnet)//3:\r\n",
        "                    print('Output Layer:')\r\n",
        "                    print('- Name: N/A')\r\n",
        "                    print('- No. of Output Nodes: '+str(supnet['W'+str(i)].shape[0]))\r\n",
        "                    print('Activation Function: softmax')\r\n",
        "                    print('--------------------------------------------------')\r\n",
        "                else:\r\n",
        "                    print('Hidden Layer '+str(i)+':')\r\n",
        "                    print('- Name: '+str(self.__hp['supnet_names'][i-1]))\r\n",
        "                    print('- No. of Hidden Nodes: '+str(supnet['W'+str(i)].shape[0]))\r\n",
        "                    if self.__hp['supnet_activations'][i-1]==relu:\r\n",
        "                        print('- Activation Function: relu')\r\n",
        "                    else:\r\n",
        "                        print('- Activation Function: sigmoid')\r\n",
        "                    print('--------------------------------------------------')\r\n",
        "        print('\\n')\r\n",
        "\r\n",
        "    def adjustLearningRate(self,learning_rate):\r\n",
        "        '''\r\n",
        "        Description: adjusts the learning rate hyperparameter\r\n",
        "\r\n",
        "        Inputs:\r\n",
        "        - learning_rate: the proposed learning rate\r\n",
        "\r\n",
        "        Returns: none\r\n",
        "        '''\r\n",
        "\r\n",
        "        #confirm the validity of the proposed learning_rate\r\n",
        "        assert type(learning_rate)==float\r\n",
        "        assert learning_rate>0\r\n",
        "\r\n",
        "        #inform the user of the current learning rate, as well as the updated learning rate\r\n",
        "        print('The current learning rate is '+str(self.__hp['learning_rate'])+'.')\r\n",
        "        self.__hp['learning_rate']=learning_rate\r\n",
        "        print('And the new learning rate is '+str(self.__hp['learning_rate'])+'.')\r\n",
        "\r\n",
        "    def adjustEpochs(self,epochs):\r\n",
        "        '''\r\n",
        "        Description: adjusts the epochs hyperparameter\r\n",
        "\r\n",
        "        Inputs:\r\n",
        "        - learning_rate: the proposed number of epochs\r\n",
        "\r\n",
        "        Returns: none\r\n",
        "        '''\r\n",
        "\r\n",
        "        #confirm the validity of the proposed number of epochs\r\n",
        "        assert type(epochs)==int\r\n",
        "        assert epochs>0\r\n",
        "\r\n",
        "        #inform the user of the current number of epochs, as well as the updated number of epochs\r\n",
        "        print('The current number of epochs is '+str(self.__hp['epochs'])+'.')\r\n",
        "        self.__hp['epochs']=epochs\r\n",
        "        print('And the new number of epochs is '+str(self.__hp['epochs'])+'.')\r\n",
        "\r\n",
        "    def adjustBatchSize(self,size):\r\n",
        "        '''\r\n",
        "        Description: adjusts the mini batch size hyperparameter\r\n",
        "\r\n",
        "        Inputs:\r\n",
        "        - learning_rate: the proposed mini batch size\r\n",
        "\r\n",
        "        Returns: none\r\n",
        "        '''\r\n",
        "\r\n",
        "        #confirm the validity of the proposed number of epochs\r\n",
        "        assert type(size)==int\r\n",
        "        assert size>0\r\n",
        "\r\n",
        "        #inform the user of the current mini batch size, as well as the updated mini batch size\r\n",
        "        print('The current mini batch size is '+str(self.__hp['mini_batch_size'])+'.')\r\n",
        "        self.__hp['mini_batch_size']=size\r\n",
        "        print('And the new mini batch size is '+str(self.__hp['mini_batch_size'])+'.')\r\n",
        "    \r\n",
        "    def adjustInitialization(self,init):\r\n",
        "        '''\r\n",
        "        Description: adjusts the initialization method hyperparameter\r\n",
        "\r\n",
        "        Inputs:\r\n",
        "        - learning_rate: the proposed intialization method\r\n",
        "\r\n",
        "        Returns: none\r\n",
        "        '''\r\n",
        "\r\n",
        "        #confirm the validity of the proposed intialization technique\r\n",
        "        assert init in ['None','He','Xavier','Other']\r\n",
        "\r\n",
        "        #inform the user of the current intialization technique, as well as the updated initialization technique\r\n",
        "        print('The current intialization tehcnique is '+str(self.__hp['initialization'])+'.')\r\n",
        "        self.__hp['initialization']=init\r\n",
        "        print('And the new initialization technique is '+str(self.__hp['initialization'])+'.')\r\n",
        "\r\n",
        "    def adjustStrips(self,strips):\r\n",
        "        '''\r\n",
        "        Description: adjusts the strips hyperparameter\r\n",
        "\r\n",
        "        Inputs:\r\n",
        "        - learning_rate: the proposed number of strips\r\n",
        "\r\n",
        "        Returns: none\r\n",
        "        '''\r\n",
        "\r\n",
        "        #confirm the validity of the proposed numbwe of strips\r\n",
        "        assert type(strips)==int\r\n",
        "        assert strips>1\r\n",
        "\r\n",
        "        #inform the user of the current number of strips, as well as the updated number of strips\r\n",
        "        print('The current number of strips into which training examples are split is '+str(self.__hp['strips'])+'.')\r\n",
        "        self.__hp['strips']=strips\r\n",
        "        print('And the new number of strips into which training examples are split is '+str(self.__hp['strips'])+'.')\r\n",
        "    \r\n",
        "    def compile(self):\r\n",
        "        '''\r\n",
        "        Description: compiles the model, trains it, and then assesses its performance on the test set\r\n",
        "\r\n",
        "        Inputs: none\r\n",
        "\r\n",
        "        Returns: none\r\n",
        "        '''\r\n",
        "\r\n",
        "        #check the validity of the relevant hyperparameters before initiating training\r\n",
        "        assert self.__hp['learning_rate']>0\r\n",
        "        assert self.__hp['epochs']>0\r\n",
        "        assert self.__hp['mini_batch_size']>0\r\n",
        "        assert self.__hp['strips']>1\r\n",
        "\r\n",
        "        #expand the activation function hyperparameters for both the sub- and the supnet\r\n",
        "        self.__hp['subnet_activations'].append(softmax)\r\n",
        "        self.__hp['supnet_activations'].append(softmax)\r\n",
        "\r\n",
        "        #exectute the relevant training\r\n",
        "        try:\r\n",
        "            self.__subnet_parameters,self.__supnet_parameters,inf_caches=train(self.__X_train,self.__Y_train,self.__hp)\r\n",
        "        except:\r\n",
        "            raise UnboundLocalError('self.__X_train and self.__Y_train referenced before assignment, meaning you have not loaded in any data. please load in a dataset and try again.')\r\n",
        "\r\n",
        "        #instantiate the networks inference parameters\r\n",
        "        self.__subnet_inf_parameters={}\r\n",
        "        for i in range(len(self.__subnet_parameters)):\r\n",
        "            self.__subnet_inf_parameters['subnet'+str(i+1)]=to_inf_params(self.__subnet_parameters['subnet'+str(i+1)],inf_caches['subnet'+str(i+1)])\r\n",
        "        self.__supnet_inf_parameters=to_inf_params(self.__supnet_parameters,inf_caches['supnet'])\r\n",
        "\r\n",
        "        #check the network's performance on the test set\r\n",
        "        print('The network\\'s performance on the test set yields an error of roughly '+str(perc_error(self.__X_test,self.__Y_test,self.__subnet_inf_parameters,self.__supnet_inf_parameters,self.__hp)))\r\n",
        "\r\n",
        "        #rectify the activation function hyperparameters for both the sub- and the supnet once training is complete\r\n",
        "        self.__hp['subnet_activations'].pop()\r\n",
        "        self.__hp['supnet_activations'].pop()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20skAuwuA1tt"
      },
      "source": [
        "<h1>FruitNet_v1.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3YbI0JgA1Ak",
        "outputId": "d2498ab9-2999-4550-e3ee-d4708a3637fe"
      },
      "source": [
        "#instantiate the network\r\n",
        "fruitnet_v1=FruitNet()\r\n",
        "\r\n",
        "#load in data from (my own) Google Drive\r\n",
        "fruitnet_v1.loadData('/content/drive/My Drive/training_set.h5','inputs','labels')\r\n",
        "\r\n",
        "#add a hidden layer of size 128 with ReLU activation to the subnet\r\n",
        "fruitnet_v1.addLayer('sub_hidden_1',128,relu)\r\n",
        "\r\n",
        "#adjust each of the relevant hyperparameters\r\n",
        "fruitnet_v1.adjustLearningRate(0.05)\r\n",
        "fruitnet_v1.adjustEpochs(256)\r\n",
        "fruitnet_v1.adjustBatchSize(128)\r\n",
        "fruitnet_v1.adjustInitialization('Other')\r\n",
        "fruitnet_v1.adjustStrips(4)\r\n",
        "\r\n",
        "#produce and view a summary of the model\r\n",
        "fruitnet_v1.modelSummary()\r\n",
        "\r\n",
        "#train the model\r\n",
        "fruitnet_v1.compile()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The current learning rate is 0.\n",
            "And the new learning rate is 0.05.\n",
            "The current number of epochs is 0.\n",
            "And the new number of epochs is 256.\n",
            "The current mini batch size is 0.\n",
            "And the new mini batch size is 128.\n",
            "The current intialization tehcnique is None.\n",
            "And the new initialization technique is Other.\n",
            "The current number of strips into which training examples are split is 0.\n",
            "And the new number of strips into which training examples are split is 4.\n",
            "==================================================\n",
            "NETWORK ARCHITECTURE SUMMARY\n",
            "==================================================\n",
            "\n",
            "\n",
            "SUBNET SUMMARY\n",
            "--------------------------------------------------\n",
            "Input Layer:\n",
            "- Name: N/A\n",
            "- No. of Input Nodes: 196\n",
            "- Activation Function: N/A\n",
            "--------------------------------------------------\n",
            "Hidden Layer 1:\n",
            "- Name: sub_hidden_1\n",
            "- No. of Hidden Nodes: 128\n",
            "- Activation Function: relu\n",
            "--------------------------------------------------\n",
            "Output Layer:\n",
            "- Name: N/A\n",
            "- No. of Output Nodes: 10\n",
            "Activation Function: softmax\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "SUPNET SUMMARY\n",
            "--------------------------------------------------\n",
            "Input Layer:\n",
            "- Name: N/A\n",
            "- No. of Input Nodes: 40\n",
            "- Activation Function: N/A\n",
            "--------------------------------------------------\n",
            "Output Layer:\n",
            "- Name: N/A\n",
            "- No. of Output Nodes: 10\n",
            "Activation Function: softmax\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "Cost after epoch 1: 0.38809186721134104\n",
            "Error on final mini batch in this epoch: 29.88071523335984\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 2: 0.32585646098937665\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 3: 0.3118337104081475\n",
            "Error on final mini batch in this epoch: 26.72612419124244\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 4: 0.2313103507885644\n",
            "Error on final mini batch in this epoch: 26.72612419124244\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 5: 0.15691877016248304\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 6: 0.11491517035249589\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 7: 0.20613642839103122\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 8: 0.20836960688533215\n",
            "Error on final mini batch in this epoch: 26.72612419124244\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 9: 0.1556979237121404\n",
            "Error on final mini batch in this epoch: 23.14550249431378\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 10: 0.17050511606922308\n",
            "Error on final mini batch in this epoch: 23.14550249431378\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 11: 0.13125194351999972\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 12: 0.17339058361570325\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 13: 0.025824459736275365\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 14: 0.3708184133462855\n",
            "Error on final mini batch in this epoch: 26.72612419124244\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 15: 0.3956189169998971\n",
            "Error on final mini batch in this epoch: 29.88071523335984\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 16: 0.08528657068917136\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 17: 0.08511418676668421\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 18: 0.33155913298720263\n",
            "Error on final mini batch in this epoch: 32.732683535398856\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 19: 0.13098325776099548\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 20: 0.0716695677458651\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 21: 0.2776302962736935\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 22: 0.07644643628980595\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 23: 0.09551546518362312\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 24: 0.11752474056125102\n",
            "Error on final mini batch in this epoch: 23.14550249431378\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 25: 0.05113159013049203\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 26: 0.06467201621797643\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 27: 0.03297931685940945\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 28: 0.10394854011953669\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 29: 0.02786657170023199\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 30: 0.05478851426687562\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 31: 0.06310393708948366\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 32: 0.020214773880703193\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 33: 0.2080012367937142\n",
            "Error on final mini batch in this epoch: 23.14550249431378\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 34: 0.018419347096609966\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 35: 0.05359210942530189\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 36: 0.11270634894784494\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 37: 0.19796518112032185\n",
            "Error on final mini batch in this epoch: 23.14550249431378\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 38: 0.23515753361744002\n",
            "Error on final mini batch in this epoch: 29.88071523335984\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 39: 0.10252707529561063\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 40: 0.039605759124414774\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 41: 0.04903145034924287\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 42: 0.16030567698960363\n",
            "Error on final mini batch in this epoch: 26.72612419124244\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 43: 0.06798128338432734\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 44: 0.05995521124043546\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 45: 0.07610348379845362\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 46: 0.11237496649420944\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 47: 0.1039548312087242\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 48: 0.04261227023652508\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 49: 0.07504165903142604\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 50: 0.014759141085519447\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 51: 0.08346208134937931\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 52: 0.18339743429506844\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 53: 0.15139356402728613\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 54: 0.02857707909413724\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 55: 0.2691147262107612\n",
            "Error on final mini batch in this epoch: 29.88071523335984\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 56: 0.012566424424425942\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 57: 0.10333882168773076\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 58: 0.05201462290706095\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 59: 0.041900258308210904\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 60: 0.0652935152845808\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 61: 0.03598496188752313\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 62: 0.014609242683641594\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 63: 0.2531560790074157\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 64: 0.1303064753431458\n",
            "Error on final mini batch in this epoch: 23.14550249431378\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 65: 0.03454180619858547\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 66: 0.021441474429605385\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 67: 0.013600187957715162\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 68: 0.013645517419453964\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 69: 0.05421415125542788\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 70: 0.0729005896353474\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 71: 0.1365188612148245\n",
            "Error on final mini batch in this epoch: 23.14550249431378\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 72: 0.08658557440401969\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 73: 0.01973331262382633\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 74: 0.020917075541083026\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 75: 0.04129716585141697\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 76: 0.24033242340724145\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 77: 0.026790517326058666\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 78: 0.15669317888880882\n",
            "Error on final mini batch in this epoch: 23.14550249431378\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 79: 0.01584344049303511\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 80: 0.03543032345251128\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 81: 0.06010680203474288\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 82: 0.0681550522688474\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 83: 0.1205608992411477\n",
            "Error on final mini batch in this epoch: 23.14550249431378\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 84: 0.10724113045999885\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 85: 0.009335975505627729\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 86: 0.08226841995404725\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 87: 0.030527563169689107\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 88: 0.0284474811477499\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 89: 0.03626380940773131\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 90: 0.0659065354277012\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 91: 0.016730118271971982\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 92: 0.01320409473422158\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 93: 0.03283070884489824\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 94: 0.046368756954614475\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 95: 0.12440287425369886\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 96: 0.03012424335518104\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 97: 0.013324681891529324\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 98: 0.011262985933009407\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 99: 0.05731174507276128\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 100: 0.03274540202743805\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 101: 0.11354396571481605\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 102: 0.09477084697675302\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 103: 0.020747699388457608\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 104: 0.018463068242395485\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 105: 0.06203855056629039\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 106: 0.04068227840514471\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 107: 0.05065136333972865\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 108: 0.019820358534687308\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 109: 0.03725288256222876\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 110: 0.010786880081514544\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 111: 0.008024130940067849\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 112: 0.0129405513543623\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 113: 0.038800828449739984\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 114: 0.027844722725459476\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 115: 0.0277139926826399\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 116: 0.021291364705083166\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 117: 0.047632056626131444\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 118: 0.1498862241441467\n",
            "Error on final mini batch in this epoch: 23.14550249431378\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 119: 0.10968719490654856\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 120: 0.03260560552931137\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 121: 0.005961321309869837\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 122: 0.02371614981274784\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 123: 0.025077111740845044\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 124: 0.02451759731123473\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 125: 0.02493562355102531\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 126: 0.061618507056202323\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 127: 0.020499770917157524\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 128: 0.009820659876159153\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 129: 0.016104707499243334\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 130: 0.029747544862244745\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 131: 0.01189913534525732\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 132: 0.007838055833815033\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 133: 0.008820430920608453\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 134: 0.11425096024124028\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 135: 0.013859640993676555\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 136: 0.007446159417382836\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 137: 0.09455113783913537\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 138: 0.0052418705703914876\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 139: 0.27534159209559284\n",
            "Error on final mini batch in this epoch: 26.72612419124244\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 140: 0.010133404749293745\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 141: 0.006759415104598965\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 142: 0.021152428859957797\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 143: 0.20645328081421788\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 144: 0.010756819688137116\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 145: 0.031071896088456324\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 146: 0.008980167684887757\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 147: 0.029161536490450748\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 148: 0.013143194976911758\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 149: 0.04059940880739275\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 150: 0.014343879077569352\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 151: 0.02095781373537955\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 152: 0.08120546061404048\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 153: 0.020951878205432675\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 154: 0.008797347406187575\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 155: 0.01521801468180455\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 156: 0.16086757554119213\n",
            "Error on final mini batch in this epoch: 23.14550249431378\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 157: 0.04940822325047038\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 158: 0.020309003662363923\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 159: 0.011766407118047233\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 160: 0.03196594068540628\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 161: 0.04048743030614449\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 162: 0.025636457431356185\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 163: 0.02454202411400062\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 164: 0.03264377501892641\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 165: 0.0336075622224364\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 166: 0.039390476971586416\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 167: 0.00818466272674306\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 168: 0.03610322913646601\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 169: 0.025760406479171598\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 170: 0.0252216277204134\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 171: 0.010077929754231564\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 172: 0.03274253098928292\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 173: 0.07853005640682723\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 174: 0.17604523004802625\n",
            "Error on final mini batch in this epoch: 23.14550249431378\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 175: 0.015196965506043603\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 176: 0.010274929817363217\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 177: 0.14965906004197022\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 178: 0.010605022232864872\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 179: 0.006068420490621824\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 180: 0.03528003689556024\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 181: 0.032412714139041635\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 182: 0.03242106703019584\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 183: 0.011278032946860996\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 184: 0.015802877799972255\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 185: 0.010761647279038472\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 186: 0.01333458895390079\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 187: 0.010902097954915048\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 188: 0.28653510479852734\n",
            "Error on final mini batch in this epoch: 23.14550249431378\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 189: 0.13141634473234193\n",
            "Error on final mini batch in this epoch: 23.14550249431378\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 190: 0.005260813355558489\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 191: 0.08055389778811657\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 192: 0.021201625467404162\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 193: 0.019977122880335565\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 194: 0.056210229997992744\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 195: 0.03801167386930872\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 196: 0.006838921319897879\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 197: 0.06617950520805731\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 198: 0.04949580922386081\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 199: 0.011025808802200273\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 200: 0.019779301309330928\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 201: 0.05422928796979991\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 202: 0.012795994845166149\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 203: 0.0847427333629541\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 204: 0.01492357600042653\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 205: 0.008761004906583339\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 206: 0.1411005378981332\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 207: 0.03361100401637739\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 208: 0.0916178356185978\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 209: 0.012811059855782661\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 210: 0.005584499866799866\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 211: 0.20564190161437532\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 212: 0.013346415314638647\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 213: 0.010945464754911169\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 214: 0.038929426898550674\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 215: 0.3196035419124721\n",
            "Error on final mini batch in this epoch: 26.72612419124244\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 216: 0.023409640481430995\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 217: 0.010263268712047463\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 218: 0.03094522478014147\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 219: 0.014957850549075707\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 220: 0.03305723389021153\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 221: 0.020465658125952302\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 222: 0.023527843040572868\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 223: 0.08192229274663869\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 224: 0.009214547192001437\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 225: 0.0259493143483516\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 226: 0.12498268785784852\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 227: 0.029957485335054334\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 228: 0.02679579205551219\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 229: 0.010364810457028209\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 230: 0.031008437138663136\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 231: 0.0891966797111058\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 232: 0.04695756217268608\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 233: 0.0055820119510418525\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 234: 0.019705076458353003\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 235: 0.04788535218930891\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 236: 0.3948072575421664\n",
            "Error on final mini batch in this epoch: 29.88071523335984\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 237: 0.10479563512823617\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 238: 0.009468608237080922\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 239: 0.01794964575592709\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 240: 0.02967443426423538\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 241: 0.021106065548967955\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 242: 0.02454154945638595\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 243: 0.1222478896692123\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 244: 0.0376414027047391\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 245: 0.02406305235151995\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 246: 0.006141537104266382\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 247: 0.018138845059381008\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 248: 0.08673976094661917\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 249: 0.12098759925164201\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 250: 0.01708577795688235\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 251: 0.04724551130083647\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 252: 0.008434781241185659\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 253: 0.015709450192246933\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 254: 0.029364380144191578\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 255: 0.007330821959976451\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 256: 0.03381013238254956\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "The network's performance on the test set yields an error of roughly 29.847795410899124\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}