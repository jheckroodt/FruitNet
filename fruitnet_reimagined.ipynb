{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fruitnet_reimagined.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R93IVx_rQBlV"
      },
      "source": [
        "<h1>Libraries</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xc-O8sSqQ63m"
      },
      "source": [
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import h5py\r\n",
        "\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 381,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHWTZPd4QGFX"
      },
      "source": [
        "<h1>Mounting the Drive</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHtTp8XAREHX",
        "outputId": "6676e365-2f8a-4e17-d358-e9d59807309d"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 382,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ppeRGiA_-U1"
      },
      "source": [
        "<h1>Dataset Operations</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-F8CNw5AAzr"
      },
      "source": [
        "def build_dataset():\r\n",
        "    training_set=h5py.File('/content/drive/My Drive/training_set.h5','r')\r\n",
        "    X=training_set['inputs'][:]\r\n",
        "    Y=training_set['labels'][:]\r\n",
        "    training_set.close()\r\n",
        "    return X,Y\r\n",
        "\r\n",
        "def build_mini_batches(X,Y,size):\r\n",
        "    X=X.reshape(X.shape[0],X.shape[1],X.shape[2])\r\n",
        "    X=X.reshape(X.shape[0],X.shape[1]*X.shape[2])\r\n",
        "    X=X.T\r\n",
        "    m=X.shape[1]\r\n",
        "    assert size<=m\r\n",
        "    permutation=np.random.permutation(m)\r\n",
        "    X=X[:,permutation]\r\n",
        "    Y=Y[:,permutation]\r\n",
        "    mini_batches=[]\r\n",
        "    whole_batches=m//size\r\n",
        "    for t in range(whole_batches):\r\n",
        "        X_mini_batch=X[:,t*size:(t+1)*size]\r\n",
        "        Y_mini_batch=Y[:,t*size:(t+1)*size]\r\n",
        "        mini_batches.append((X_mini_batch,Y_mini_batch))\r\n",
        "    if m%size!=0:\r\n",
        "        X_mini_batch=X[:,whole_batches*size:m]\r\n",
        "        Y_mini_batch=Y[:,whole_batches*size:m]\r\n",
        "        mini_batches.append((X_mini_batch,Y_mini_batch))\r\n",
        "    return mini_batches\r\n",
        "\r\n",
        "def reshuffle_split_mini_batches(X,Y,hp):\r\n",
        "    mini_batches=build_mini_batches(X,Y,hp['mini_batch_size'])\r\n",
        "    split_mini_batches=[]\r\n",
        "    for (Xt,Yt) in mini_batches:\r\n",
        "        split_mini_batches.append((split_into_strips(Xt,hp['strips']),Yt))\r\n",
        "    return split_mini_batches\r\n",
        "\r\n",
        "def split_into_strips(Xt,strips):\r\n",
        "    if int(np.sqrt(Xt.shape[0]))%strips==0:\r\n",
        "        width=Xt.shape[0]//strips\r\n",
        "        split_mini_batch=[]\r\n",
        "        for i in range(strips):\r\n",
        "            split_mini_batch.append(Xt[i*width:(i+1)*width,:])\r\n",
        "        return tuple(split_mini_batch)\r\n",
        "    else:\r\n",
        "        raise ValueError('you have elected a number of strips that results in strips of unequal width. please re-enter the number of strips you\\'d like, and try again.')"
      ],
      "execution_count": 383,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOFZ2dieQJ87"
      },
      "source": [
        "<h1>Initialization Operations</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0qF7s3nQ48u"
      },
      "source": [
        "def initialize_parameters(layer_dims,initialization):\r\n",
        "    L=len(layer_dims)\r\n",
        "    parameters={}\r\n",
        "    if initialization=='None':\r\n",
        "        for l in range(1,L):\r\n",
        "            parameters['W'+str(l)]=0.01*np.random.rand(layer_dims[l],layer_dims[l-1])\r\n",
        "            parameters['gamma'+str(l)]=np.ones((layer_dims[l],1))\r\n",
        "            parameters['beta'+str(l)]=np.zeros((layer_dims[l],1))\r\n",
        "    elif initialization=='He':\r\n",
        "        for l in range(1,L):\r\n",
        "            const=np.sqrt(2/layer_dims[l-1])\r\n",
        "            parameters['W'+str(l)]=const*np.random.rand(layer_dims[l],layer_dims[l-1])\r\n",
        "            parameters['gamma'+str(l)]=np.ones((layer_dims[l],1))\r\n",
        "            parameters['beta'+str(l)]=np.zeros((layer_dims[l],1))\r\n",
        "    elif initialization=='Xavier':\r\n",
        "        for l in range(1,L):\r\n",
        "            const=np.sqrt(1/layer_dims[l-1])\r\n",
        "            parameters['W'+str(l)]=const*np.random.rand(layer_dims[l],layer_dims[l-1])\r\n",
        "            parameters['gamma'+str(l)]=np.ones((layer_dims[l],1))\r\n",
        "            parameters['beta'+str(l)]=np.zeros((layer_dims[l],1))\r\n",
        "    elif initialization=='Other':\r\n",
        "        for l in range(1,L):\r\n",
        "            const=np.sqrt(2/(layer_dims[l]+layer_dims[l-1]))\r\n",
        "            parameters['W'+str(l)]=const*np.random.rand(layer_dims[l],layer_dims[l-1])\r\n",
        "            parameters['gamma'+str(l)]=np.ones((layer_dims[l],1))\r\n",
        "            parameters['beta'+str(l)]=np.zeros((layer_dims[l],1))\r\n",
        "    return parameters\r\n",
        "\r\n",
        "def build_nets(split_mini_batch,hp):\r\n",
        "    X=split_mini_batch[0][0]\r\n",
        "    Y=split_mini_batch[1]\r\n",
        "    subnet_parameters={}\r\n",
        "    if hp['subnet_hidden']==[]:\r\n",
        "        layer_dims=[X.shape[0],Y.shape[0]]\r\n",
        "    else:\r\n",
        "        layer_dims=[X.shape[0]]+hp['subnet_hidden']+[Y.shape[0]]\r\n",
        "    for i in range(hp['strips']):\r\n",
        "        subnet_parameters['subnet'+str(i+1)]=initialize_parameters(layer_dims,hp['initialization'])\r\n",
        "    if hp['supnet_hidden']==[]:\r\n",
        "        layer_dims=[hp['strips']*Y.shape[0],Y.shape[0]]\r\n",
        "    else:\r\n",
        "        layer_dims=[hp['strips']*Y.shape[0]]+hp['supnet_hidden']+[Y.shape[0]]\r\n",
        "    supnet_parameters=initialize_parameters(layer_dims,hp['initialization'])\r\n",
        "    return subnet_parameters,supnet_parameters\r\n",
        "\r\n",
        "def normalize_inputs(X,epsilon=1e-8):\r\n",
        "    return X/255\r\n"
      ],
      "execution_count": 384,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZNy-1qcQONr"
      },
      "source": [
        "<h1>Network Operations</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ttbdnRhQ5gZ"
      },
      "source": [
        "def sigmoid(z,backward=False):\r\n",
        "    if backward:\r\n",
        "        return (1/(1+np.exp(-z)))*(1-(1/(1+np.exp(-z))))\r\n",
        "    else:\r\n",
        "        return 1/(1+np.exp(-z))\r\n",
        "\r\n",
        "def relu(z,backward=False):\r\n",
        "    if backward:\r\n",
        "        return np.where(z<0,0,1)\r\n",
        "    else:\r\n",
        "        return np.where(z<0,0,z)\r\n",
        "\r\n",
        "def softmax(z):\r\n",
        "    magnitude=np.sum(np.exp(z),axis=0,keepdims=True)\r\n",
        "    return np.exp(z)/magnitude\r\n",
        "\r\n",
        "def concat(z,classes,m=None):\r\n",
        "    if m==None:\r\n",
        "        subnet_dA=[]\r\n",
        "        for i in range(z.shape[0]//classes):\r\n",
        "            subnet_dA.append(z[i*classes:(i+1)*classes,:])\r\n",
        "        return subnet_dA\r\n",
        "    else:\r\n",
        "        new_z=np.zeros((len(z)*classes,m))\r\n",
        "        for i in range(len(z)):\r\n",
        "            new_z[i*classes:(i+1)*classes,:]=z[i]\r\n",
        "        return new_z\r\n",
        "\r\n",
        "def propagate_forward(X,parameters,activations,epsilon=1e-8):\r\n",
        "    A_prev=X\r\n",
        "    m=X.shape[1]\r\n",
        "    cache_list=[]\r\n",
        "    L=len(parameters)//3\r\n",
        "    for l in range(L):\r\n",
        "        Z=np.dot(parameters['W'+str(l+1)],A_prev)\r\n",
        "        mu=np.mean(Z,axis=1,keepdims=True)\r\n",
        "        std=np.var(Z,axis=1,keepdims=True)\r\n",
        "        Zhat=(Z-mu)/np.sqrt(std+epsilon)\r\n",
        "        Ztilde=parameters['gamma'+str(l+1)]*Zhat+parameters['beta'+str(l+1)]\r\n",
        "        cache_list.append((Ztilde,Zhat,mu,Z,std,A_prev))\r\n",
        "        A_prev=activations[l](Ztilde)\r\n",
        "    return cache_list,A_prev\r\n",
        "\r\n",
        "def compute_cost(AL,Y):\r\n",
        "    m=Y.shape[1]\r\n",
        "    loss=-np.sum(Y*np.log(AL),axis=0,keepdims=True)\r\n",
        "    return np.squeeze(np.sum(loss,axis=1,keepdims=True))/m\r\n",
        "\r\n",
        "def propagate_backward(Y,AL,cache_list,parameters,activations,epsilon=1e-8):\r\n",
        "    grads={}\r\n",
        "    m=Y.shape[1]\r\n",
        "    L=len(parameters)//3\r\n",
        "    for l in reversed(range(L)):\r\n",
        "        (Ztilde,Zhat,mu,Z,std,A_prev)=cache_list[l]\r\n",
        "        if l==L-1:\r\n",
        "            dZtilde_loss=AL-Y\r\n",
        "            dZtilde=dZtilde_loss/m\r\n",
        "        else:\r\n",
        "            dZtilde=dA*activations[l](Ztilde,backward=True)\r\n",
        "        dZhat=dZtilde*parameters['gamma'+str(l+1)]\r\n",
        "        dgamma=np.sum(dZtilde*Zhat,axis=1,keepdims=True)\r\n",
        "        dbeta=np.sum(dZtilde,axis=1,keepdims=True)\r\n",
        "        dstd=np.sum(dZhat*((mu-Z)/(2*(std+epsilon)**(3/2))),axis=1,keepdims=True)\r\n",
        "        dmu=-np.sum(dZhat*(1/np.sqrt(std+epsilon)),axis=1,keepdims=True)\r\n",
        "        dZ=dZhat*(1/np.sqrt(std+epsilon))+(2*dstd*(Z-mu))/m+dmu/m\r\n",
        "        dW=np.dot(m*dZ,A_prev.T)\r\n",
        "        dA=np.dot(parameters['W'+str(l+1)].T,dZ)\r\n",
        "        grads['dgamma'+str(l+1)]=dgamma\r\n",
        "        grads['dbeta'+str(l+1)]=dbeta\r\n",
        "        grads['dW'+str(l+1)]=dW\r\n",
        "    return grads\r\n",
        "\r\n",
        "def update_parameters(parameters,grads,learning_rate,epoch_num):\r\n",
        "    lrate=learning_rate/(1+(1e-1)*epoch_num)\r\n",
        "    L=len(parameters)//3\r\n",
        "    for l in range(L):\r\n",
        "        parameters['W'+str(l+1)]=parameters['W'+str(l+1)]-lrate*grads['dW'+str(l+1)]\r\n",
        "        parameters['gamma'+str(l+1)]=parameters['gamma'+str(l+1)]-lrate*grads['dgamma'+str(l+1)]\r\n",
        "        parameters['beta'+str(l+1)]=parameters['beta'+str(l+1)]-lrate*grads['dbeta'+str(l+1)]\r\n",
        "    return parameters"
      ],
      "execution_count": 385,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZbYySY_RK1C"
      },
      "source": [
        "<h1>Compilation Operation(s)</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TdIHADcRfMZ"
      },
      "source": [
        "def train(X,Y,hp):\r\n",
        "    normalized_X=normalize_inputs(X)\r\n",
        "    split_mini_batches=reshuffle_split_mini_batches(normalized_X,Y,hp)\r\n",
        "    subnet_parameters,supnet_parameters=build_nets(split_mini_batches[0],hp)\r\n",
        "    for e in range(hp['epochs']):\r\n",
        "        for (Xt,Yt) in split_mini_batches:\r\n",
        "            all_subnet_AL=[]\r\n",
        "            all_caches={}\r\n",
        "            for i in range(len(Xt)):\r\n",
        "                subnet_cache,subnet_AL=propagate_forward(Xt[i],subnet_parameters['subnet'+str(i+1)],hp['subnet_activations'])\r\n",
        "                all_subnet_AL.append(subnet_AL)\r\n",
        "                all_caches['subnet'+str(i+1)]=subnet_cache\r\n",
        "                subnet_grads=propagate_backward(Yt,subnet_AL,subnet_cache,subnet_parameters['subnet'+str(i+1)],hp['subnet_activations'])\r\n",
        "                subnet_parameters['subnet'+str(i+1)]=update_parameters(subnet_parameters['subnet'+str(i+1)],subnet_grads,hp['learning_rate'],e)\r\n",
        "            Xt_concat=concat(all_subnet_AL,Yt.shape[0],m=Yt.shape[1])\r\n",
        "            supnet_cache,supnet_AL=propagate_forward(Xt_concat,supnet_parameters,hp['supnet_activations'])\r\n",
        "            all_caches['supnet']=supnet_cache\r\n",
        "            supnet_grads=propagate_backward(Yt,supnet_AL,supnet_cache,supnet_parameters,hp['supnet_activations'])\r\n",
        "            supnet_parameters=update_parameters(supnet_parameters,supnet_grads,hp['learning_rate'],e)\r\n",
        "        Y_hat=np.where(supnet_AL==np.amax(supnet_AL,axis=0,keepdims=True),1,0)\r\n",
        "        print('--------------------------------------------------')\r\n",
        "        print('Cost after epoch '+str(e+1)+': '+str(compute_cost(supnet_AL,Yt)))\r\n",
        "        print('Error on final mini batch in this epoch: '+str(100*(np.linalg.norm(Yt-Y_hat)/np.sqrt(2*Yt.shape[1]))))\r\n",
        "        print('--------------------------------------------------')\r\n",
        "        split_mini_batches=reshuffle_split_mini_batches(normalized_X,Y,hp)\r\n",
        "    return subnet_parameters,supnet_parameters,all_caches"
      ],
      "execution_count": 386,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_v4vXOiISLD2"
      },
      "source": [
        "<h1>Inference Operations</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjD2RemBSNso"
      },
      "source": [
        "def to_inf_params(parameters,cache_list,epsilon=1e-8):\r\n",
        "    inf_params={}\r\n",
        "    L=len(parameters)//3\r\n",
        "    for l in range(L):\r\n",
        "        (Ztilde,Zhat,mu,Z,std,A_prev)=cache_list[l]\r\n",
        "        inf_params['W'+str(l+1)]=parameters['W'+str(l+1)]\r\n",
        "        inf_params['gamma'+str(l+1)]=parameters['gamma'+str(l+1)]/np.sqrt(std+epsilon)\r\n",
        "        inf_params['beta'+str(l+1)]=parameters['beta'+str(l+1)]-mu*(parameters['gamma'+str(l+1)]/np.sqrt(std+epsilon))\r\n",
        "    return inf_params\r\n",
        "\r\n",
        "def inference_forward(X,parameters,activations,epsilon=1e-8):\r\n",
        "    A_prev=X\r\n",
        "    L=len(parameters)//3\r\n",
        "    for l in range(L):\r\n",
        "        Z=np.dot(parameters['W'+str(l+1)],A_prev)\r\n",
        "        Ztilde=parameters['gamma'+str(l+1)]*Z+parameters['beta'+str(l+1)]\r\n",
        "        A_prev=activations[l](Ztilde)\r\n",
        "    return A_prev\r\n",
        "\r\n",
        "def perc_error(X,Y,subnet_parameters,supnet_parameters,hp):\r\n",
        "    normalized_X=normalize_inputs(X)\r\n",
        "    [(Xt,Yt)]=reshuffle_split_mini_batches(normalized_X,Y,{'mini_batch_size':Y.shape[1],'strips':hp['strips']})\r\n",
        "    all_subnet_AL=[]\r\n",
        "    for i in range(len(Xt)):\r\n",
        "        subnet_AL=inference_forward(Xt[i],subnet_parameters['subnet'+str(i+1)],hp['subnet_activations'])\r\n",
        "        all_subnet_AL.append(subnet_AL)\r\n",
        "    Xt_concat=concat(all_subnet_AL,Yt.shape[0],m=Yt.shape[1])\r\n",
        "    supnet_AL=inference_forward(Xt_concat,supnet_parameters,hp['supnet_activations'])\r\n",
        "    Y_hat=np.where(supnet_AL==np.amax(supnet_AL,axis=0,keepdims=True),1,0)\r\n",
        "    assert Y_hat.shape==Y.shape\r\n",
        "    return 100*(np.linalg.norm(Yt-Y_hat)/np.sqrt(2*Yt.shape[1]))"
      ],
      "execution_count": 387,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5B9J3oIaRfdU"
      },
      "source": [
        "<h1>FruitNet API</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BTBJwgUN24m"
      },
      "source": [
        "hyperparameters={'learning_rate':0.05,\r\n",
        "                 'epochs':256,\r\n",
        "                 'mini_batch_size':128,\r\n",
        "                 'initialization':'Other',\r\n",
        "                 'strips':int(4),\r\n",
        "                 'subnet_hidden':[128],\r\n",
        "                 'subnet_activations':[relu,softmax],\r\n",
        "                 'supnet_hidden':[],\r\n",
        "                 'supnet_activations':[softmax]}\r\n",
        "\r\n",
        "class FruitNet():\r\n",
        "\r\n",
        "    def __init__(self):\r\n",
        "\r\n",
        "        #instantiate hyperparameters\r\n",
        "        self.__hp={'learning_rate':0,\r\n",
        "                   'epochs':0,\r\n",
        "                   'mini_batch_size':0,\r\n",
        "                   'initialization':None,\r\n",
        "                   'strips':0,\r\n",
        "                   'subnet_names':[],\r\n",
        "                   'subnet_hidden':[],\r\n",
        "                   'subnet_activations':[],\r\n",
        "                   'supnet_names':[],\r\n",
        "                   'supnet_hidden':[],\r\n",
        "                   'supnet_activations':[]}\r\n",
        "    \r\n",
        "    def __raw_shuffle__(self,X,Y):\r\n",
        "\r\n",
        "        #permute [1,2,...,m]\r\n",
        "        permutation=np.random.permutation(X.shape[0])\r\n",
        "\r\n",
        "        #shuffle X and Y in accordance with the above permutation\r\n",
        "        shuffled_X=X[permutation,:,:,:]\r\n",
        "        shuffled_Y=Y[:,permutation]\r\n",
        "\r\n",
        "        return shuffled_X,shuffled_Y\r\n",
        "\r\n",
        "    def loadData(self,filename,features,labels,split=0.05):\r\n",
        "\r\n",
        "        #retrieve data\r\n",
        "        try:\r\n",
        "            training_set=h5py.File(filename,'r')\r\n",
        "            X=training_set[features][:]\r\n",
        "            Y=training_set[labels][:]\r\n",
        "            training_set.close()\r\n",
        "        except:\r\n",
        "            raise NameError('you\\'ve entered an incorrect h5py filename, feature dataset name, or label dataset name, when loading in your data.')\r\n",
        "\r\n",
        "        #confirm validity of X dataset shape\r\n",
        "        assert len(X.shape)==4\r\n",
        "        assert X.shape[3]==1\r\n",
        "\r\n",
        "        #confirm validity of Y dataset shape\r\n",
        "        assert len(Y.shape)==2\r\n",
        "        assert X.shape[0]==Y.shape[1]\r\n",
        "\r\n",
        "        #shuffle datasets (maintaining correspondence)\r\n",
        "        shuffled_X,shuffled_Y=self.__raw_shuffle__(X,Y)\r\n",
        "\r\n",
        "        #establish cross-validation test set (and, therefore, training set) sizes\r\n",
        "        m=X.shape[0]\r\n",
        "        self.__test_quant=int(split*m)\r\n",
        "        self.__train_quant=m-self.__test_quant\r\n",
        "\r\n",
        "        #create cross-validation test set and training set using the above sizes\r\n",
        "        self.__X_train=shuffled_X[self.__test_quant:self.__test_quant+self.__train_quant,:,:,:]\r\n",
        "        self.__Y_train=shuffled_Y[:,self.__test_quant:self.__test_quant+self.__train_quant]\r\n",
        "        self.__X_test=shuffled_X[:self.__test_quant,:,:,:]\r\n",
        "        self.__Y_test=shuffled_Y[:,:self.__test_quant]\r\n",
        "\r\n",
        "    def __retrieve_single_split_example__(self,strips,view_data=True):\r\n",
        "\r\n",
        "        #select a random sample number\r\n",
        "        try:\r\n",
        "            sample=np.random.randint(0,self.__test_quant)\r\n",
        "        except:\r\n",
        "            raise UnboundLocalError('self.__test_quant has referenced before assignment, meaning you have not yet loaded in any data. please load in data and try again.')\r\n",
        "\r\n",
        "        #select the sample from the test set and split into regions\r\n",
        "        X=np.zeros((1,self.__X_test.shape[1],self.__X_test.shape[2],1))\r\n",
        "        Y=np.zeros((self.__Y_test.shape[0],1))\r\n",
        "        X[0,:,:,0]=self.__X_test[sample,:,:,0]\r\n",
        "        Y[:,0]=self.__Y_test[:,sample]\r\n",
        "        [(Xt,Yt)]=reshuffle_split_mini_batches(X,Y,{'mini_batch_size':1,'strips':strips})\r\n",
        "\r\n",
        "        #check why the function has been called and return accordingly\r\n",
        "        if view_data:\r\n",
        "            return X,Xt,Y\r\n",
        "        else:\r\n",
        "            return (Xt,Yt)\r\n",
        "\r\n",
        "\r\n",
        "    def viewData(self,strips):\r\n",
        "\r\n",
        "        #confirm the validity of the number of strips\r\n",
        "        assert type(strips)==int\r\n",
        "        assert strips>1\r\n",
        "\r\n",
        "        #retrieve (split) item of data (from test set) corresponding to random_sample\r\n",
        "        X,Xt,Y=self.__retrieve_single_split_example__(strips)\r\n",
        "\r\n",
        "        #plot the sample, as well as the strips into which we've split it, and the corresponding label\r\n",
        "        fig=plt.figure()\r\n",
        "        axes=fig.subplots(strips+1,1)\r\n",
        "        axes[0].imshow(X[0,:,:,0])\r\n",
        "        for i in range(strips):\r\n",
        "            axes[i+1].imshow(Xt[i].reshape(X.shape[1]//strips,X.shape[2]))\r\n",
        "        print('The label corresponding to the above item of data (read from left to right) is given by:\\n'+str(Y.reshape(Y.shape[0],)))\r\n",
        "    \r\n",
        "    def addLayer(self,name,n_H,activation,net='sub'):\r\n",
        "        \r\n",
        "        #confirm the validity of name, n_H, activation, and supnet\r\n",
        "        assert type(name)==str\r\n",
        "        assert type(n_H)==int\r\n",
        "        assert n_H>0\r\n",
        "        assert activation in [relu,sigmoid]\r\n",
        "        assert type(net)==str\r\n",
        "\r\n",
        "        #append the hyperparameters in accordance with the function parameters\r\n",
        "        if net=='sup':\r\n",
        "            assert name not in self.__hp['supnet_names']\r\n",
        "            self.__hp['supnet_names'].append(name)\r\n",
        "            self.__hp['supnet_hidden'].append(n_H)\r\n",
        "            self.__hp['supnet_activations'].append(activation)\r\n",
        "        elif net=='sub':\r\n",
        "            assert name not in self.__hp['subnet_names']\r\n",
        "            self.__hp['subnet_names'].append(name)\r\n",
        "            self.__hp['subnet_hidden'].append(n_H)\r\n",
        "            self.__hp['subnet_activations'].append(activation)\r\n",
        "        else:\r\n",
        "            raise ValueError('you have not selected a valid network to add a layer to. please re-enter the net string and try again.')\r\n",
        "    \r\n",
        "    def __decoy_parameters__(self):\r\n",
        "\r\n",
        "        #extract an example from the test set to preserve computational efficiency\r\n",
        "        single_example_mini_batch=self.__retrieve_single_split_example__(self.__hp['strips'],view_data=False)\r\n",
        "\r\n",
        "        #generate and return relevant parameters (NOT as class attributes, though)\r\n",
        "        return build_nets(single_example_mini_batch,self.__hp)\r\n",
        "\r\n",
        "    def modelSummary(self):\r\n",
        "\r\n",
        "        #confirm that enough hyperparameters have been specified in order for a summary to be generated\r\n",
        "        assert self.__hp['strips']>0\r\n",
        "\r\n",
        "        #retrieve the parameters whose summary we wish to produce\r\n",
        "        try:\r\n",
        "            subnet=self.__subnet_parameters['subnet1']\r\n",
        "            supnet=self.__supnet_parameters\r\n",
        "        except:\r\n",
        "            subnets,supnet=self.__decoy_parameters__()\r\n",
        "            subnet=subnets['subnet1']\r\n",
        "        \r\n",
        "        #produce model summary header\r\n",
        "        print('==================================================')\r\n",
        "        print('NETWORK ARCHITECTURE SUMMARY')\r\n",
        "        print('==================================================\\n\\n')\r\n",
        "\r\n",
        "        #produce subnet summary\r\n",
        "        print('SUBNET SUMMARY')\r\n",
        "        print('--------------------------------------------------')\r\n",
        "        for i in range(len(subnet)//3+1):\r\n",
        "            if i==0:\r\n",
        "                print('Input Layer:')\r\n",
        "                print('- Name: N/A')\r\n",
        "                print('- No. of Input Nodes: '+str(subnet['W1'].shape[1]))\r\n",
        "                print('- Activation Function: N/A')\r\n",
        "                print('--------------------------------------------------')\r\n",
        "            else:\r\n",
        "                if i==len(subnet)//3:\r\n",
        "                    print('Output Layer:')\r\n",
        "                    print('- Name: N/A')\r\n",
        "                    print('- No. of Output Nodes: '+str(subnet['W'+str(i)].shape[0]))\r\n",
        "                    print('Activation Function: softmax')\r\n",
        "                    print('--------------------------------------------------')\r\n",
        "                else:\r\n",
        "                    print('Hidden Layer '+str(i)+':')\r\n",
        "                    print('- Name: '+str(self.__hp['subnet_names'][i-1]))\r\n",
        "                    print('- No. of Hidden Nodes: '+str(subnet['W'+str(i)].shape[0]))\r\n",
        "                    if self.__hp['subnet_activations'][i-1]==relu:\r\n",
        "                        print('- Activation Function: relu')\r\n",
        "                    else:\r\n",
        "                        print('- Activation Function: sigmoid')\r\n",
        "                    print('--------------------------------------------------')\r\n",
        "        print('\\n')\r\n",
        "\r\n",
        "        #produce supnet summary\r\n",
        "        print('SUPNET SUMMARY')\r\n",
        "        print('--------------------------------------------------')\r\n",
        "        for i in range(len(supnet)//3+1):\r\n",
        "            if i==0:\r\n",
        "                print('Input Layer:')\r\n",
        "                print('- Name: N/A')\r\n",
        "                print('- No. of Input Nodes: '+str(supnet['W1'].shape[1]))\r\n",
        "                print('- Activation Function: N/A')\r\n",
        "                print('--------------------------------------------------')\r\n",
        "            else:\r\n",
        "                if i==len(supnet)//3:\r\n",
        "                    print('Output Layer:')\r\n",
        "                    print('- Name: N/A')\r\n",
        "                    print('- No. of Output Nodes: '+str(supnet['W'+str(i)].shape[0]))\r\n",
        "                    print('Activation Function: softmax')\r\n",
        "                    print('--------------------------------------------------')\r\n",
        "                else:\r\n",
        "                    print('Hidden Layer '+str(i)+':')\r\n",
        "                    print('- Name: '+str(self.__hp['supnet_names'][i-1]))\r\n",
        "                    print('- No. of Hidden Nodes: '+str(supnet['W'+str(i)].shape[0]))\r\n",
        "                    if self.__hp['supnet_activations'][i-1]==relu:\r\n",
        "                        print('- Activation Function: relu')\r\n",
        "                    else:\r\n",
        "                        print('- Activation Function: sigmoid')\r\n",
        "                    print('--------------------------------------------------')\r\n",
        "        print('\\n')\r\n",
        "\r\n",
        "    def adjustLearningRate(self,learning_rate):\r\n",
        "\r\n",
        "        #confirm the validity of the proposed learning_rate\r\n",
        "        assert type(learning_rate)==float\r\n",
        "        assert learning_rate>0\r\n",
        "\r\n",
        "        #inform the user of the current learning rate, as well as the updated learning rate\r\n",
        "        print('The current learning rate is '+str(self.__hp['learning_rate'])+'.')\r\n",
        "        self.__hp['learning_rate']=learning_rate\r\n",
        "        print('And the new learning rate is '+str(self.__hp['learning_rate'])+'.')\r\n",
        "\r\n",
        "    def adjustEpochs(self,epochs):\r\n",
        "\r\n",
        "        #confirm the validity of the proposed number of epochs\r\n",
        "        assert type(epochs)==int\r\n",
        "        assert epochs>0\r\n",
        "\r\n",
        "        #inform the user of the current number of epochs, as well as the updated number of epochs\r\n",
        "        print('The current number of epochs is '+str(self.__hp['epochs'])+'.')\r\n",
        "        self.__hp['epochs']=epochs\r\n",
        "        print('And the new number of epochs is '+str(self.__hp['epochs'])+'.')\r\n",
        "\r\n",
        "    def adjustBatchSize(self,size):\r\n",
        "\r\n",
        "        #confirm the validity of the proposed number of epochs\r\n",
        "        assert type(size)==int\r\n",
        "        assert size>0\r\n",
        "\r\n",
        "        #inform the user of the current mini batch size, as well as the updated mini batch size\r\n",
        "        print('The current mini batch size is '+str(self.__hp['mini_batch_size'])+'.')\r\n",
        "        self.__hp['mini_batch_size']=size\r\n",
        "        print('And the new mini batch size is '+str(self.__hp['mini_batch_size'])+'.')\r\n",
        "    \r\n",
        "    def adjustInitialization(self,init):\r\n",
        "\r\n",
        "        #confirm the validity of the proposed intialization technique\r\n",
        "        assert init in ['None','He','Xavier','Other']\r\n",
        "\r\n",
        "        #inform the user of the current intialization technique, as well as the updated initialization technique\r\n",
        "        print('The current intialization tehcnique is '+str(self.__hp['initialization'])+'.')\r\n",
        "        self.__hp['initialization']=init\r\n",
        "        print('And the new initialization technique is '+str(self.__hp['initialization'])+'.')\r\n",
        "\r\n",
        "    def adjustStrips(self,strips):\r\n",
        "\r\n",
        "        #confirm the validity of the proposed numbwe of strips\r\n",
        "        assert type(strips)==int\r\n",
        "        assert strips>1\r\n",
        "\r\n",
        "        #inform the user of the current number of strips, as well as the updated number of strips\r\n",
        "        print('The current number of strips into which training examples are split is '+str(self.__hp['strips'])+'.')\r\n",
        "        self.__hp['strips']=strips\r\n",
        "        print('And the new number of strips into which training examples are split is '+str(self.__hp['strips'])+'.')\r\n",
        "    \r\n",
        "    def compile(self):\r\n",
        "\r\n",
        "        #check the validity of the relevant hyperparameters before initiating training\r\n",
        "        assert self.__hp['learning_rate']>0\r\n",
        "        assert self.__hp['epochs']>0\r\n",
        "        assert self.__hp['mini_batch_size']>0\r\n",
        "        assert self.__hp['strips']>1\r\n",
        "\r\n",
        "        #expand the activation function hyperparameters for both the sub- and the supnet\r\n",
        "        self.__hp['subnet_activations'].append(softmax)\r\n",
        "        self.__hp['supnet_activations'].append(softmax)\r\n",
        "\r\n",
        "        #exectute the relevant training\r\n",
        "        try:\r\n",
        "            self.__subnet_parameters,self.__supnet_parameters,inf_caches=train(self.__X_train,self.__Y_train,self.__hp)\r\n",
        "        except:\r\n",
        "            raise UnboundLocalError('self.__X_train and self.__Y_train referenced before assignment, meaning you have not loaded in any data. please load in a dataset and try again.')\r\n",
        "\r\n",
        "        #instantiate the networks inference parameters\r\n",
        "        self.__subnet_inf_parameters={}\r\n",
        "        for i in range(len(self.__subnet_parameters)):\r\n",
        "            self.__subnet_inf_parameters['subnet'+str(i+1)]=to_inf_params(self.__subnet_parameters['subnet'+str(i+1)],inf_caches['subnet'+str(i+1)])\r\n",
        "        self.__supnet_inf_parameters=to_inf_params(self.__supnet_parameters,inf_caches['supnet'])\r\n",
        "\r\n",
        "        #check the network's performance on the test set\r\n",
        "        print('The network\\'s performance on the test set yields an error of roughly '+str(perc_error(self.__X_test,self.__Y_test,self.__subnet_inf_parameters,self.__supnet_inf_parameters,self.__hp)))\r\n",
        "\r\n",
        "        #rectify the activation function hyperparameters for both the sub- and the supnet once training is complete\r\n",
        "        self.__hp['subnet_activations'].pop()\r\n",
        "        self.__hp['supnet_activations'].pop()\r\n",
        "\r\n",
        "    def saveModel(self):\r\n",
        "        pass"
      ],
      "execution_count": 388,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPbYmJLQukld"
      },
      "source": [
        "<h1>FruitNet_v1.0</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nh0euOPTukN3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a15b7dda-3a8b-4878-d87d-d741c7e8674b"
      },
      "source": [
        "#instantiate the network\r\n",
        "fruitnet_v1=FruitNet()\r\n",
        "\r\n",
        "#load in data from (my own) Google Drive\r\n",
        "fruitnet_v1.loadData('/content/drive/My Drive/training_set.h5','inputs','labels')\r\n",
        "\r\n",
        "#add a hidden layer of size 128 with ReLU activation to the subnet\r\n",
        "fruitnet_v1.addLayer('sub_hidden_1',128,relu)\r\n",
        "\r\n",
        "#adjust each of the relevant hyperparameters\r\n",
        "fruitnet_v1.adjustLearningRate(0.05)\r\n",
        "fruitnet_v1.adjustEpochs(256)\r\n",
        "fruitnet_v1.adjustBatchSize(128)\r\n",
        "fruitnet_v1.adjustInitialization('Other')\r\n",
        "fruitnet_v1.adjustStrips(4)\r\n",
        "\r\n",
        "#produce and view a summary of the model\r\n",
        "fruitnet_v1.modelSummary()\r\n",
        "\r\n",
        "#train the model\r\n",
        "fruitnet_v1.compile()"
      ],
      "execution_count": 390,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The current learning rate is 0.\n",
            "And the new learning rate is 0.05.\n",
            "The current number of epochs is 0.\n",
            "And the new number of epochs is 256.\n",
            "The current mini batch size is 0.\n",
            "And the new mini batch size is 128.\n",
            "The current intialization tehcnique is None.\n",
            "And the new initialization technique is Other.\n",
            "The current number of strips into which training examples are split is 0.\n",
            "And the new number of strips into which training examples are split is 4.\n",
            "==================================================\n",
            "NETWORK ARCHITECTURE SUMMARY\n",
            "==================================================\n",
            "\n",
            "\n",
            "SUBNET SUMMARY\n",
            "--------------------------------------------------\n",
            "Input Layer:\n",
            "- Name: N/A\n",
            "- No. of Input Nodes: 196\n",
            "- Activation Function: N/A\n",
            "--------------------------------------------------\n",
            "Hidden Layer 1:\n",
            "- Name: sub_hidden_1\n",
            "- No. of Hidden Nodes: 128\n",
            "- Activation Function: relu\n",
            "--------------------------------------------------\n",
            "Output Layer:\n",
            "- Name: N/A\n",
            "- No. of Output Nodes: 10\n",
            "Activation Function: softmax\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "SUPNET SUMMARY\n",
            "--------------------------------------------------\n",
            "Input Layer:\n",
            "- Name: N/A\n",
            "- No. of Input Nodes: 40\n",
            "- Activation Function: N/A\n",
            "--------------------------------------------------\n",
            "Output Layer:\n",
            "- Name: N/A\n",
            "- No. of Output Nodes: 10\n",
            "Activation Function: softmax\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "Cost after epoch 1: 0.31208240776349017\n",
            "Error on final mini batch in this epoch: 23.14550249431378\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 2: 0.26877324311221906\n",
            "Error on final mini batch in this epoch: 26.72612419124244\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 3: 0.2709442051657067\n",
            "Error on final mini batch in this epoch: 29.88071523335984\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 4: 0.41521854080199183\n",
            "Error on final mini batch in this epoch: 35.35533905932737\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 5: 0.21711900555915337\n",
            "Error on final mini batch in this epoch: 26.72612419124244\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 6: 0.08847539328799163\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 7: 0.14258697703510168\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 8: 0.10447483847834148\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 9: 0.10021313068668884\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 10: 0.2125535457029946\n",
            "Error on final mini batch in this epoch: 26.72612419124244\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 11: 0.04704819356435519\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 12: 0.23516082486754164\n",
            "Error on final mini batch in this epoch: 23.14550249431378\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 13: 0.17471293897923582\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 14: 0.12781870482774696\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 15: 0.18853210947625465\n",
            "Error on final mini batch in this epoch: 23.14550249431378\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 16: 0.03019419823315758\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 17: 0.37044223237708357\n",
            "Error on final mini batch in this epoch: 32.732683535398856\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 18: 0.03681470277697655\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 19: 0.07713453572712393\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 20: 0.071485154068553\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 21: 0.05391430213342731\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 22: 0.04735478286741166\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 23: 0.12598398800481522\n",
            "Error on final mini batch in this epoch: 23.14550249431378\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 24: 0.044695930410106825\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 25: 0.030601740547834332\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 26: 0.07339127462631659\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 27: 0.17481406497312252\n",
            "Error on final mini batch in this epoch: 23.14550249431378\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 28: 0.05401002081230543\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 29: 0.11976738559719964\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 30: 0.08748617872371946\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 31: 0.06627068839533637\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 32: 0.08744429107917943\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 33: 0.0316194322604725\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 34: 0.20817341518675808\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 35: 0.09581207380426113\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 36: 0.05579442619955212\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 37: 0.06640743216620885\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 38: 0.03557469876980174\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 39: 0.06672341198540309\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 40: 0.1000516403768539\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 41: 0.027923082688557442\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 42: 0.06264136478009612\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 43: 0.05926879918214732\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 44: 0.013170739965163523\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 45: 0.047562434616455714\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 46: 0.09267935859916321\n",
            "Error on final mini batch in this epoch: 23.14550249431378\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 47: 0.1416717009628196\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 48: 0.038250236681102834\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 49: 0.13702436522168068\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 50: 0.03629823037669256\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 51: 0.043090462660819694\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 52: 0.025001529193931088\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 53: 0.013278943772395524\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 54: 0.0827540105427723\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 55: 0.02804032938931394\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 56: 0.042385150195976336\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 57: 0.023387150674896075\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 58: 0.01040407384695069\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 59: 0.0675027616546172\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 60: 0.048781044576643014\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 61: 0.07286869079546296\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 62: 0.05690512870096774\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 63: 0.030337966776721185\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 64: 0.17362480763936347\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 65: 0.08220450086177712\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 66: 0.012235934578825974\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 67: 0.0563495949418269\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 68: 0.06785888055185622\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 69: 0.058145241075274155\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 70: 0.012050062843646379\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 71: 0.14812525026250598\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 72: 0.014171589085058963\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 73: 0.028506135504427207\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 74: 0.1625990299480568\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 75: 0.040114248466445325\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 76: 0.07529533872182916\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 77: 0.030610084863364164\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 78: 0.09797799667425389\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 79: 0.12084896936694343\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 80: 0.05164640914129919\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 81: 0.07769467137567221\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 82: 0.13937648983050252\n",
            "Error on final mini batch in this epoch: 23.14550249431378\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 83: 0.08220624276293241\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 84: 0.01001830137598415\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 85: 0.0939140531461748\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 86: 0.06152277322699156\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 87: 0.02912403783362396\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 88: 0.07435509915629443\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 89: 0.04669219967451795\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 90: 0.12311743342911263\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 91: 0.016455812097751422\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 92: 0.034583446541899474\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 93: 0.04413267310896424\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 94: 0.07171586326223453\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 95: 0.021877169730439055\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 96: 0.04005215980661038\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 97: 0.03293383472304059\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 98: 0.02074368507676206\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 99: 0.0175881958381593\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 100: 0.01796013209820801\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 101: 0.0681073293231106\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 102: 0.0415382361309936\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 103: 0.02613102441141771\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 104: 0.030913778495635962\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 105: 0.08487701885538199\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 106: 0.09359604762203887\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 107: 0.0052950205707819956\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 108: 0.07402764021437981\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 109: 0.02369729392040124\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 110: 0.00805334445178312\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 111: 0.006832024524934469\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 112: 0.14013161563592375\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 113: 0.0316929708640896\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 114: 0.04122930181696605\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 115: 0.02370406683510243\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 116: 0.017042209890638583\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 117: 0.01553632862766607\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 118: 0.026451471534273862\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 119: 0.039421343819657464\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 120: 0.026010529292358666\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 121: 0.10100914649961992\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 122: 0.026811358814262283\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 123: 0.02280888844790351\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 124: 0.04063204370222102\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 125: 0.04270002503611712\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 126: 0.012840258613555535\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 127: 0.0119137372575691\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 128: 0.12806028457043825\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 129: 0.04032419821248422\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 130: 0.015232474923984765\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 131: 0.03306971048260986\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 132: 0.06735528499061484\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 133: 0.06992416258394014\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 134: 0.05561562217780249\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 135: 0.013253867594077335\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 136: 0.008804373715162178\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 137: 0.024690684825488703\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 138: 0.022425111995614207\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 139: 0.010075450996221398\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 140: 0.01920111871030598\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 141: 0.019900095816270353\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 142: 0.0068727099245420985\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 143: 0.019039445706092534\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 144: 0.05314267357488576\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 145: 0.032477575471710395\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 146: 0.08304333150780767\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 147: 0.013445992126092616\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 148: 0.014498079363005236\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 149: 0.017800544987461753\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 150: 0.011612185377595107\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 151: 0.016549287049931995\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 152: 0.005575941626580781\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 153: 0.028754455013208592\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 154: 0.017784401246746337\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 155: 0.10900770170063454\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 156: 0.07026214330688797\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 157: 0.025168327735879903\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 158: 0.09490338507280764\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 159: 0.03611538776203851\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 160: 0.11522919272164167\n",
            "Error on final mini batch in this epoch: 23.14550249431378\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 161: 0.007974621368121786\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 162: 0.0687111181189607\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 163: 0.04932953696157676\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 164: 0.021685669045491685\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 165: 0.026401556681814552\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 166: 0.06633375622999833\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 167: 0.07346666083335252\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 168: 0.014209270685571937\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 169: 0.02560728283780952\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 170: 0.08002564100696997\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 171: 0.13061472857865067\n",
            "Error on final mini batch in this epoch: 23.14550249431378\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 172: 0.026065204926462266\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 173: 0.09313146986846166\n",
            "Error on final mini batch in this epoch: 23.14550249431378\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 174: 0.0701800470809976\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 175: 0.07193163230192981\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 176: 0.012786389686745578\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 177: 0.07886631407237153\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 178: 0.016308792716044414\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 179: 0.055038683737996155\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 180: 0.01169171174962388\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 181: 0.08036309143174439\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 182: 0.013958953244414204\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 183: 0.028918798800128095\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 184: 0.1048993295533697\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 185: 0.11213089667543187\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 186: 0.01702216164318324\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 187: 0.014921426802573735\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 188: 0.033542179920273904\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 189: 0.026262920500772845\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 190: 0.022652327899862885\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 191: 0.0062378074223478\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 192: 0.1644796766002218\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 193: 0.0630957731094204\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 194: 0.03294643156823611\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 195: 0.042999109245891357\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 196: 0.011966987269991704\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 197: 0.0068226178556879844\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 198: 0.05590312409987647\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 199: 0.03960175427355616\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 200: 0.015526344839661105\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 201: 0.06329881094621442\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 202: 0.0631008509472107\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 203: 0.011381612856505395\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 204: 0.007856112381779487\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 205: 0.057458310014211755\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 206: 0.00784651627803603\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 207: 0.008377940058938082\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 208: 0.00720744681077494\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 209: 0.062057184498254724\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 210: 0.05037443697186828\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 211: 0.08971470306798465\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 212: 0.06803175831745613\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 213: 0.030572145175039415\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 214: 0.010489422963677733\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 215: 0.007865389841822236\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 216: 0.022524681844822016\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 217: 0.014170523911505013\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 218: 0.060034680111409666\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 219: 0.019694896278938754\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 220: 0.018609843121658108\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 221: 0.00717933766179841\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 222: 0.016264619455030247\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 223: 0.011920940921654069\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 224: 0.15853113947803538\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 225: 0.017494435508467073\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 226: 0.02389120375078748\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 227: 0.03385232646307202\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 228: 0.05363056271918687\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 229: 0.053999847352079036\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 230: 0.018715366070973377\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 231: 0.05265930528959214\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 232: 0.01713746286441082\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 233: 0.00894180780659141\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 234: 0.20698078484100169\n",
            "Error on final mini batch in this epoch: 23.14550249431378\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 235: 0.010414968517438745\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 236: 0.02013006913051116\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 237: 0.03661041593162222\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 238: 0.017162263304320226\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 239: 0.011110910088845025\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 240: 0.011546001473319493\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 241: 0.019730396043869686\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 242: 0.05180411258710371\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 243: 0.02763828518024732\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 244: 0.05778620839892856\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 245: 0.14206317729888748\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 246: 0.1864199223084191\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 247: 0.07565096667225668\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 248: 0.04267052340855489\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 249: 0.02599392648047677\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 250: 0.00857560671167437\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 251: 0.006806826588377275\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 252: 0.01641005883376955\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 253: 0.015919577638054436\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 254: 0.010344947802852274\n",
            "Error on final mini batch in this epoch: 0.0\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 255: 0.09429250005592701\n",
            "Error on final mini batch in this epoch: 13.36306209562122\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Cost after epoch 256: 0.07888406071288658\n",
            "Error on final mini batch in this epoch: 18.89822365046136\n",
            "--------------------------------------------------\n",
            "The network's performance on the test set yields an error of roughly 24.710536429033883\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}